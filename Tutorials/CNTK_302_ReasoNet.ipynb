{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# CNTK 302: ReasoNet for Machine Comprehension\n",
    "\n",
    "## Introduction and Background\n",
    "\n",
    "In CNTK 204 tutorial, we show how to generate a sequence from an input sequence of data. We have introduced the \"attention mechanism\" which results in superior performance. Research has shown how the human brain pays selective attention to certain parts of the enormous amount of information that comes our way by filtering out extraneous data that is not currently of import for the task at hand. One can extend this concept to neural networks. This approach – commonly referred to as “attention” is used as a building block for other machine understanding tasks. \n",
    "\n",
    "This tutorial is an advancement over attention based mechanisms shown earlier. The Reasoning Network [ReasoNet](https://posenhuang.github.io/papers/reasonet_iclr_2017.pdf) has achieved industry-leading results in reading comprehension. The ReasoNet model is a leading model with ExactMatch (EM) and F1 scores (being 73.42 and 81.75, respectively at the time of creation of this tutorial) on the Stanford Question Answering Dataset [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/). \n",
    "\n",
    "**Goal:**\n",
    "\n",
    "The goal of this tutorial is to show how we can teach machines to read, process and comprehend natural language documents. Genuine reading comprehension is extremely challenging as it requires understanding of the documents and performing reasoning involving different context. Enabling a machine to answer a question based on provided passage has gained significant attention enabled by powerful deep learning models.\n",
    "\n",
    "**Problem**\n",
    "\n",
    "In the context of [CNN data](https://github.com/deepmind/rc-data) where there are 4 components: (1) query (q), (2) document (d), (3) candidate list (a) and (4) the true answer (A), in this tutorial, we train a model that enables machine comprehension with state-of-the-art performance where we find out the answer for a question given a paragraph text. We will be training our model on triplets as a collection of query, passage and answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Data preparation\n",
    "\n",
    "### Download data\n",
    "The data can be downloaded via the follwoing two links: [link1](https://drive.google.com/uc?export=download&id=0BwmD_VLjROrfTTljRDVZMFJnVWM) or [link2](https://github.com/deepmind/rc-data)\n",
    "The downloaded data is packaged as a gz file and needs to be reformatted prior to use in this tutorial. After unpacking the file, we will generate three folders: training, test, and validation. Each folder contains several file each consisting of a paragraph of text, a question, the corresponding the answer to the questions and a list of entities. Here is an example of one data instance: \n",
    "\n",
    "> __*1:*__  http://web.archive.org/web/20150731215720id_/http://edition.cnn.com/2015/04/07/sport/wladimir-klitschko-ukraine-crisis-boxing/<br/>\n",
    ">__*2:*__  <br/>\n",
    "\n",
    "> **Paragraph**\n",
    "\n",
    ">>__*3:*__  @entity3 ( @entity2 ) @entity1 heavyweight boxing champion @entity0 has an important title defense coming up , but his thoughts continue to be dominated by the ongoing fight for democracy in @entity8 . speaking to @entity2 from his @entity3 training base ahead of the april 25 showdown with @entity12 challenger @entity11 in @entity13 , @entity0 said the crisis in his homeland has left him shocked and upset . \" my country is unfortunately suffering in the war with @entity18 -- not that @entity8 tried to give any aggression to any other nation , in this particular case @entity18 , unfortunately it 's the other way around , \" @entity0 told @entity2 . \" i never thought that our brother folk is going to have war with us , so that @entity8 and @entity18 are going to be divided with blood , \" he added . \" unfortunately , we do n't know how far it 's going to go and how worse it 's going to get . the aggression , in the military presence of ( @entity18 ) soldiers and military equipment in my country , @entity8 , is upsetting . \" @entity0 is the reigning @entity33 , @entity34 , @entity35 and @entity36 champion and has , alongside older brother @entity37 , dominated the heavyweight division in the 21st century . @entity37 , who retired from boxing in 2013 , is a prominent figure in @entity8 politics . the 43 - year - old has led the @entity43 since 2010 and was elected mayor of @entity45 in may last year . tensions in the former @entity48 state remain high despite a ceasefire agreed in february as @entity50 , led by @entity52 chancellor @entity51 and president of france @entity53 , tries to broker a peace deal between the two sides . the crisis in @entity8 began in november 2013 when former president @entity58 scuttled a trade deal with the @entity60 in favor of forging closer economic ties with @entity18 . the move triggered a wave of anti-government protests which came to a head @entity45 's @entity67 in february 2014 when clashes between protesters and government security forces left around 100 dead . the following month , @entity18 troops entered @entity8 's @entity74 peninsula before @entity18 president @entity75 completed the annexation of @entity74 -- a move denounced by most of the world as illegitimate -- after citizens of the region had voted in favor of leaving @entity8 in a referendum . more than 5,000 people have been killed in the conflict to date . \" people are dying in @entity8 every single day , \" @entity0 said . \" i do not want to see it , nobody wants to see it ... it 's hard to believe these days something like that in @entity50 -- and @entity8 is @entity50 -- can happen . \" but with the backing of the international community , @entity0 is confident @entity8 can forge a democratic future rather than slide back towards a @entity48 - era style dictatorship . \" i really wish and want this conflict to be solved and it can only be solved with @entity98 help , \" he said . \" @entity8 is looking forward to becoming a democratic country and live under @entity98 democracy . this is our decision and this is our will to get what we want . \" if somebody wants to try to put ( us ) back to the @entity48 times and be part of the @entity108 , we disagree with that . we want to be in freedom . \" we have achieved many things in moving forward and showed to the world that we do not want to live under a dictatorship . \" @entity0 , whose comments were made as part of a wide - ranging interview for @entity2 's @entity118 series , is routinely kept abreast of developments in @entity8 by brother @entity37 but also returns home whenever he can . \" as much time as i can spend , i am there in the @entity8 . it 's not like i am getting the news from mass media and making my own adjustments and judgments on what 's going on . it 's an actual presence and understanding from the inside ... it obviously affects my life , it affects the life of my family . \" the 39 - year - old and his fiancée @entity137 celebrated happier times last december when the @entity12 actress gave birth to a baby daughter , @entity142 . \" i need to get used to it that i 'm a father , which is really exciting . i hope i 'm going to have a big family with multiple kids , \" he said . @entity0 is n't sure when he 'll finally hang up his gloves . \" i do n't know how long i can last ... motivation and health have to be there to continue . \" but after leaving almost all his boxing opponents battered and bruised -- the @entity8 is seeking an impressive 18th consecutive title defense against @entity11 -- @entity0 is keen to carry on fighting his own and his country 's corner in the opposite way outside the ring . \" i just really want that we 'll have less violence in the world ... i hope in peace we can do anything , but if we have war then it 's definitely going to leave us dull and numb . \" watch @entity0 's @entity118 interview on @entity2 's @entity165 on wednesday april 8 at 1130 , 1245 , 1445 , 2130 , 2245 and 2345 and thursday april 9 at 0445 ( all times gmt ) and here online .<br/>\n",
    ">>__*4:*__  <br/>\n",
    "\n",
    "> **Question**\n",
    ">>__*5:*__  @placeholder faces @entity12 challenger @entity11 in @entity13 on april 25<br/>\n",
    ">>__*6:*__  <br/>\n",
    "\n",
    "> **Answer**\n",
    ">>__*7:*__  @entity0<br/>\n",
    ">>__*8:*__  <br/>\n",
    "\n",
    "> **Entity mapping in pargraph and query**\n",
    ">>__*9:*__  @entity118:Human to Hero<br/>\n",
    ">>__*10:*__  @entity13:New York<br/>\n",
    ">>__*11:*__  @entity137:Hayden Panettiere<br/>\n",
    ">>__*12:*__  @entity12:American<br/>\n",
    ">>__*13:*__  @entity3:Miami<br/>\n",
    ">>__*14:*__  @entity2:CNN<br/>\n",
    ">>__*15:*__  @entity1:World<br/>\n",
    ">>__*16:*__  @entity0:Klitschko<br/>\n",
    ">>__*17:*__  @entity11:Bryant Jennings<br/>\n",
    ">>__*18:*__  @entity8:Ukraine<br/>\n",
    ">>__*19:*__  @entity53:Francois Hollande<br/>\n",
    ">>__*20:*__  @entity52:German<br/>\n",
    ">>__*21:*__  @entity51:Angela Merkel<br/>\n",
    ">>__*22:*__  @entity50:Europe<br/>\n",
    ">>__*23:*__  @entity75:Vladimir Putin<br/>\n",
    ">>__*24:*__  @entity74:Crimea<br/>\n",
    ">>__*25:*__  @entity58:Victor Yanukovych<br/>\n",
    ">>__*26:*__  @entity33:IBF<br/>\n",
    ">>__*27:*__  @entity35:WBO<br/>\n",
    ">>__*28:*__  @entity34:WBA<br/>\n",
    ">>__*29:*__  @entity37:Vitali<br/>\n",
    ">>__*30:*__  @entity36:IBO<br/>\n",
    ">>__*31:*__  @entity18:Russian<br/>\n",
    ">>__*32:*__  @entity98:Western<br/>\n",
    ">>__*33:*__  @entity108:former Soviet Union<br/>\n",
    ">>__*34:*__  @entity142:Kaya<br/>\n",
    ">>__*35:*__  @entity165:World Sport program<br/>\n",
    ">>__*36:*__  @entity45:Kiev<br/>\n",
    ">>__*37:*__  @entity43:Ukrainian Democratic Alliance for Reform<br/>\n",
    ">>__*38:*__  @entity67:Maidan Square<br/>\n",
    ">>__*39:*__  @entity48:Soviet<br/>\n",
    ">>__*40:*__  @entity60:European Union<br/>\n",
    "\n",
    "We will use the following block of code to download and merge each folder of files into a single file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All necessary data are downloaded to ../Examples/LanguageUnderstanding/ReasoNet/Data\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import sys\n",
    "import tarfile\n",
    "import shutil\n",
    "\n",
    "def merge_files(folder, target):\n",
    "  if os.path.exists(target):\n",
    "    return\n",
    "  count = 0\n",
    "  all_files = os.listdir(folder)\n",
    "  print(\"Start to merge {0} files under folder {1} as {2}\".format(len(all_files), folder, target))\n",
    "    \n",
    "  for f in all_files:\n",
    "    txt=os.path.join(folder, f)\n",
    "    if os.path.isfile(txt):\n",
    "      with open(txt, encoding='utf-8') as sample:\n",
    "        content = sample.readlines()\n",
    "        context = content[2].strip()\n",
    "        query = content[4].strip()\n",
    "        answer = content[6].strip()\n",
    "        entities = []\n",
    "        for k in range(8, len(content)):\n",
    "          entities += [ content[k].strip() ]\n",
    "        with open(target, 'a', encoding='utf-8') as output:\n",
    "          output.write(\"{0}\\t{1}\\t{2}\\t{3}\\n\".format(query, answer, context, \"\\t\".join(entities)))\n",
    "    count+=1\n",
    "    if count%1000==0:\n",
    "      sys.stdout.write(\".\")\n",
    "      sys.stdout.flush()\n",
    "  print()\n",
    "  print(\"Finished to merge {0}\".format(target))\n",
    "\n",
    "def download_cnn(target=\".\"):\n",
    "  if os.path.exists(os.path.join(target, \"cnn\")):\n",
    "    shutil.rmtree(os.path.join(target, \"cnn\"))\n",
    "  if not os.path.exists(target):\n",
    "    os.makedirs(target)\n",
    "    \n",
    "  url=\"https://drive.google.com/uc?export=download&id=0BwmD_VLjROrfTTljRDVZMFJnVWM\"\n",
    "  print(\"Start to download CNN data from {0} to {1}\".format(url, target))\n",
    "    \n",
    "  pre_request = requests.get(url)\n",
    "  confirm_match = re.search(r\"confirm=(.{4})\", pre_request.content.decode(\"utf-8\"))\n",
    "  confirm_url = url + \"&confirm=\" + confirm_match.group(1)\n",
    "  download_request = requests.get(confirm_url, cookies=pre_request.cookies)\n",
    "  tar = tarfile.open(mode=\"r:gz\", fileobj=io.BytesIO(download_request.content))\n",
    "  tar.extractall(target)\n",
    "  print(\"Finished to download {0} to {1}\".format(url, target))\n",
    "\n",
    "def file_exists(src):\n",
    "  return (os.path.isfile(src) and os.path.exists(src))\n",
    "\n",
    "data_root = \"../Examples/LanguageUnderstanding/ReasoNet/Data\"\n",
    "raw_train_data=os.path.join(data_root, \"cnn/training.txt\")\n",
    "raw_test_data=os.path.join(data_root, \"cnn/test.txt\")\n",
    "raw_validation_data=os.path.join(data_root, \"cnn/validation.txt\")\n",
    "if not (file_exists(raw_train_data) and file_exists(raw_test_data) and file_exists(raw_validation_data)):\n",
    "  download_cnn(data_root)\n",
    "\n",
    "merge_files(os.path.join(data_root, \"cnn/questions/training\"), raw_train_data)\n",
    "merge_files(os.path.join(data_root, \"cnn/questions/test\"), raw_test_data)\n",
    "merge_files(os.path.join(data_root, \"cnn/questions/validation\"), raw_validation_data)\n",
    "print(\"All necessary data are downloaded to {0}\".format(data_root))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Convert to CNTK Text Format\n",
    "\n",
    "In order to take advantage of the scalable readers bundled with CNTK, we need to convert the original data into a column separated format [CNTK text format](https://github.com/Microsoft/CNTK/wiki/BrainScript-CNTKTextFormat-Reader). \n",
    "There are 5 columns/streams in the conveted CTF data file, including context, query, entity indication, label, entity ids. Here is a snippnet of the converted CTF output for the above example input,\n",
    "\n",
    ">  0 |Q 12:1   |C 4:1    |E 1 |L 0 |EID 4:1<br/>\n",
    ">  |Q 1739:1 |C 626:1  |E 0 |L 0 |EID 2:1<br/>\n",
    ">  |Q 14:1   |C 2:1    |E 1 |L 0 |EID 1:1<br/>\n",
    ">  |Q 5453:1 |C 625:1  |E 0 |L 0 |EID 3:1<br/>\n",
    ">  |Q 13:1   |C 1:1    |E 1 |L 0 |EID 9:1<br/>\n",
    ">  |Q 594:1  |C 7562:1 |E 0 |L 0 |EID 2:1<br/>\n",
    ">  |Q 15:1   |C 5284:1 |E 0 |L 0 |EID 4:1<br/>\n",
    ">  |Q 600:1  |C 1245:1 |E 0 |L 0 |EID 14:1<br/>\n",
    ">  |Q 1307:1 |C 3:1    |E 1 |L 1 |EID 13:1<br/>\n",
    ">  |Q 1309:1 |C 616:1  |E 0 |L 0 |EID 15:1<br/>\n",
    ">  |C 620:1  |E 0 |L 0  EID 3:1<br/>\n",
    ">  |C 927:1  |E 0 |L 0 |EID 20:1<br/>\n",
    ">  |C 1115:1 |E 0 |L 0 |EID 9:1<br/>\n",
    ">  |C 1017:1 |E 0 |L 0 |EID 20:1<br/>\n",
    ">  |C 1067:1 |E 0 |L 0 |EID 3:1<br/>\n",
    ">  |C 650:1  |E 0 |L 0 |EID 2:1<br/>\n",
    ">  |C 587:1  |E 0 |L 0 |EID 9:1<br/>\n",
    ">  |C 613:1  |E 0 |L 0 |EID 20:1<br/>\n",
    ">  |C 608:1  |E 0 |L 0 |EID 20:1<br/>\n",
    ">  |C 2892:1 |E 0 |L 0 |EID 9:1<br/>\n",
    ">  |C 1015:1 |E 0 |L 0 |EID 3:1<br/>\n",
    ">  |C 589:1  |E 0 |L 0 |EID 35:1<br/>\n",
    ">  |C 615:1  |E 0 |L 0 |EID 36:1<br/>\n",
    ">  |C 2814:1 |E 0 |L 0 |EID 37:1<br/>\n",
    ">  |C 617:1  |E 0 |L 0 |EID 39:1<br/>\n",
    ">  |C 586:1  |E 0 |L 0 |EID 40:1<br/>\n",
    ">  |C 2090:1 |E 0 |L 0 |EID 40:1<br/>\n",
    ">  |C 1057:1 |E 0 |L 0 |EID 9:1<br/>\n",
    ">  |C 597:1  |E 0 |L 0 |EID 44:1<br/>\n",
    ">  |C 2054:1 |E 0 |L 0 |EID 47:1<br/>\n",
    "\n",
    "The first column is the sequence id, 0. The second is the features of Query delimited by `|Q`, the third is the features of Context delimited by '|C', and the fourth is a boolean to indicate if that word in the Context is an entity, the fifth is the Label which indicate if that word in the context is the answer. The last is the ID of entities in the context.\n",
    "\n",
    "The code below performs the conversion. \n",
    "\n",
    "**Note**: The downloading and conversion can take upto 30 min and requires 11 GB of local disc space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data conversion finished.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import math\n",
    "import functools\n",
    "import numpy as np\n",
    "\n",
    "class WordFreq:\n",
    "  def __init__(self, word, id, freq):\n",
    "    self.word = word\n",
    "    self.id = id\n",
    "    self.freq = freq\n",
    "\n",
    "class Vocabulary:\n",
    "  \"\"\"Build word vocabulary with frequency\"\"\"\n",
    "  def __init__(self, name):\n",
    "    self.name = name\n",
    "    self.size = 0\n",
    "    self.__dict = {}\n",
    "    self.__has_index = False\n",
    "    self.__reverse = {}\n",
    "\n",
    "  def push(self, word):\n",
    "    if word in self.__dict:\n",
    "      self.__dict[word].freq += 1\n",
    "    else:\n",
    "      self.__dict[word] = WordFreq(word, len(self.__dict), 1)\n",
    "\n",
    "  def build_index(self, max_size):\n",
    "    def word_cmp(x, y):\n",
    "      if x.freq == y.freq :\n",
    "        return (x.word > y.word) - (x.word < y.word)\n",
    "      else:\n",
    "        return x.freq - y.freq\n",
    "\n",
    "    items = sorted(self.__dict.values(), key=functools.cmp_to_key(word_cmp), reverse=True)\n",
    "    if len(items)>max_size:\n",
    "      del items[max_size:]\n",
    "    self.size=len(items)\n",
    "    self.__dict.clear()\n",
    "    for it in items:\n",
    "      it.id = len(self.__dict)\n",
    "      self.__dict[it.word] = it\n",
    "    self.__has_index = True\n",
    "\n",
    "  def save(self, dst):\n",
    "    if not self.__has_index:\n",
    "      self.build_index(sys.maxsize)\n",
    "    if self.name != None:\n",
    "      dst.write(\"{0}\\t{1}\\n\".format(self.name, self.size))\n",
    "    for it in sorted(self.__dict.values(), key=lambda it:it.id):\n",
    "      dst.write(\"{0}\\t{1}\\t{2}\\n\".format(it.word, it.id, it.freq))\n",
    "\n",
    "  def load(self, src):\n",
    "    line = src.readline()\n",
    "    if line == \"\":\n",
    "      return\n",
    "    line = line.rstrip('\\n')\n",
    "    head = line.split()\n",
    "    max_size = sys.maxsize\n",
    "    if len(head) == 2:\n",
    "      self.name = head[0]\n",
    "      max_size = int(head[1])\n",
    "    cnt = 0\n",
    "    while cnt < max_size:\n",
    "      line = src.readline()\n",
    "      if line == \"\":\n",
    "        break\n",
    "      line = line.rstrip('\\n')\n",
    "      items = line.split()\n",
    "      self.__dict[items[0]] = WordFreq(items[0], int(items[1]), int(items[2]))\n",
    "      cnt += 1\n",
    "    self.size = len(self.__dict)\n",
    "    self.__has_index = True\n",
    "\n",
    "  def __getitem__(self, key):\n",
    "    if key in self.__dict:\n",
    "      return self.__dict[key]\n",
    "    else:\n",
    "      return None\n",
    "\n",
    "  def values(self):\n",
    "    return self.__dict.values()\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.size\n",
    "\n",
    "  def __contains__(self, q):\n",
    "    return q in self.__dict\n",
    "\n",
    "  def lookup_by_id(self, id):\n",
    "    if self.__reverse is None or len(self.__reverse) == 0:\n",
    "      self.__reverse = {}\n",
    "      for item in self.__dict.items():\n",
    "        self.__reverse[item[1].id]=item[0]\n",
    "    return self.__reverse[id]\n",
    "\n",
    "  @staticmethod\n",
    "  def is_cnn_entity(word):\n",
    "    return word.startswith('@entity') or word.startswith('@placeholder')\n",
    "\n",
    "  @staticmethod\n",
    "  def load_vocab(vocab_src):\n",
    "    \"\"\"\n",
    "    Loa vocabulary from file.\n",
    "\n",
    "    Args:\n",
    "      vocab_src (`str`): the file stored with the vocabulary data\n",
    "      \n",
    "    Returns:\n",
    "      :class:`Vocabulary`: Vocabulary of the entities\n",
    "      :class:`Vocabulary`: Vocabulary of the words\n",
    "    \"\"\"\n",
    "    word_vocab = Vocabulary(\"WordVocab\")\n",
    "    entity_vocab = Vocabulary(\"EntityVocab\")\n",
    "    with open(vocab_src, 'r', encoding='utf-8') as src:\n",
    "      entity_vocab.load(src)\n",
    "      word_vocab.load(src)\n",
    "    return entity_vocab, word_vocab\n",
    "\n",
    "  @staticmethod\n",
    "  def build_vocab(input_src, vocab_dst, max_size=50000):\n",
    "    \"\"\"\n",
    "    Build vocabulary from raw corpus file.\n",
    "\n",
    "    Args:\n",
    "      input_src (`str`): the path of the corpus file\n",
    "      vocab_dst (`str`): the path of the vocabulary file to save the built vocabulary\n",
    "      max_size (`int`): the maxium size of the word vocabulary\n",
    "    Returns:\n",
    "      :class:`Vocabulary`: Vocabulary of the entities\n",
    "      :class:`Vocabulary`: Vocabulary of the words\n",
    "    \"\"\"\n",
    "    # Leave the first as Unknown\n",
    "    max_size -= 1\n",
    "    word_vocab = Vocabulary(\"WordVocab\")\n",
    "    entity_vocab = Vocabulary(\"EntityVocab\")\n",
    "    linenum = 0\n",
    "    print(\"Start build vocabulary from {0} with maxium words {1}. Saved to {2}\"\\\n",
    "          .format(input_src, max_size, vocab_dst))\n",
    "    with open(input_src, 'r', encoding='utf-8') as src:\n",
    "      all_lines = src.readlines()\n",
    "      print(\"Total lines to process: {0}\".format(len(all_lines)))\n",
    "      for line in all_lines:\n",
    "        line = line.strip('\\n')\n",
    "        ans, query_words, context_words = Vocabulary.parse_corpus_line(line)\n",
    "        for q in query_words:\n",
    "          if Vocabulary.is_cnn_entity(q):\n",
    "          #if q.startswith('@'):\n",
    "            entity_vocab.push(q)\n",
    "          else:\n",
    "            word_vocab.push(q)\n",
    "        for q in context_words:\n",
    "          #if q.startswith('@'):\n",
    "          if Vocabulary.is_cnn_entity(q):\n",
    "            entity_vocab.push(q)\n",
    "          else:\n",
    "            word_vocab.push(q)\n",
    "        linenum += 1\n",
    "        if linenum%1000==0:\n",
    "          sys.stdout.write(\".\")\n",
    "          sys.stdout.flush()\n",
    "    print()\n",
    "    entity_vocab.build_index(max_size)\n",
    "    word_vocab.build_index(max_size)\n",
    "    with open(vocab_dst, 'w', encoding='utf-8') as dst:\n",
    "      entity_vocab.save(dst)\n",
    "      word_vocab.save(dst)\n",
    "    print(\"Finished to generate vocabulary from: {0}\".format(input_src))\n",
    "    return entity_vocab, word_vocab\n",
    "\n",
    "  @staticmethod\n",
    "  def parse_corpus_line(line):\n",
    "    \"\"\"\n",
    "    Parse bing corpus line to answer, query and context.\n",
    "\n",
    "    Args:\n",
    "      line (`str`): A line of text of bing corpus\n",
    "    Returns:\n",
    "      :`str`: Answer word\n",
    "      :`str[]`: Array of query words\n",
    "      :`str[]`: Array of context/passage words\n",
    "\n",
    "    \"\"\"\n",
    "    data = line.split('\\t')\n",
    "    query = data[0]\n",
    "    answer = data[1]\n",
    "    context = data[2]\n",
    "    query_words = query.split()\n",
    "    context_words = context.split()\n",
    "    return answer, query_words, context_words\n",
    "\n",
    "  def build_corpus(entities, words, corpus, output, max_seq_len=100000):\n",
    "    \"\"\"\n",
    "    Build featurized corpus and store it in CNTK Text Format.\n",
    "\n",
    "    Args:\n",
    "      entities (class:`Vocabulary`): The entities vocabulary\n",
    "      words (class:`Vocabulary`): The words vocabulary\n",
    "      corpus (`str`): The file path of the raw corpus\n",
    "      output (`str`): The file path to store the featurized corpus data file\n",
    "    \"\"\"\n",
    "    seq_id = 0\n",
    "    print(\"Start to build CTF data from: {0}\".format(corpus))\n",
    "    with open(corpus, 'r', encoding = 'utf-8') as corp:\n",
    "      with open(output, 'w', encoding = 'utf-8') as outf:\n",
    "        all_lines = corp.readlines()\n",
    "        print(\"Total lines to prcess: {0}\".format(len(all_lines)))\n",
    "        for line in all_lines:\n",
    "          line = line.strip('\\n')\n",
    "          ans, query_words, context_words = Vocabulary.parse_corpus_line(line)\n",
    "          ans_item = entities[ans]\n",
    "          query_ids = []\n",
    "          context_ids = []\n",
    "          is_entity = []\n",
    "          entity_ids = []\n",
    "          labels = []\n",
    "          pos = 0\n",
    "          answer_idx = None\n",
    "          for q in context_words:\n",
    "            if Vocabulary.is_cnn_entity(q):\n",
    "              item = entities[q]\n",
    "              context_ids += [ item.id + 1 ]\n",
    "              entity_ids += [ item.id + 1 ]\n",
    "              is_entity += [1]\n",
    "              if ans_item.id == item.id:\n",
    "                labels += [1]\n",
    "                answer_idx = pos\n",
    "              else:\n",
    "                labels += [0]\n",
    "            else:\n",
    "              item = words[q]\n",
    "              context_ids += [ (item.id + 1 + entities.size) if item != None else 0 ]\n",
    "              is_entity += [0]\n",
    "              labels += [0]\n",
    "            pos += 1\n",
    "            if (pos >= max_seq_len):\n",
    "              break\n",
    "          if answer_idx is None:\n",
    "            continue\n",
    "          for q in query_words:\n",
    "            if Vocabulary.is_cnn_entity(q):\n",
    "              item = entities[q]\n",
    "              query_ids += [ item.id + 1 ]\n",
    "            else:\n",
    "              item = words[q]\n",
    "              query_ids += [ (item.id + 1 + entities.size) if item != None else 0 ]\n",
    "          #Write featurized ids\n",
    "          outf.write(\"{0}\".format(seq_id))\n",
    "          for i in range(max(len(context_ids), len(query_ids))):\n",
    "            if i < len(query_ids):\n",
    "              outf.write(\" |Q {0}:1\".format(query_ids[i]))\n",
    "            if i < len(context_ids):\n",
    "              outf.write(\" |C {0}:1\".format(context_ids[i]))\n",
    "              outf.write(\" |E {0}\".format(is_entity[i]))\n",
    "              outf.write(\" |L {0}\".format(labels[i]))\n",
    "            if i < len(entity_ids):\n",
    "              outf.write(\" |EID {0}:1\".format(entity_ids[i]))\n",
    "            outf.write(\"\\n\")\n",
    "          seq_id += 1\n",
    "          if seq_id%1000 == 0:\n",
    "            sys.stdout.write(\".\")\n",
    "            sys.stdout.flush()\n",
    "    print()\n",
    "    print(\"Finished to build corpus from {0}\".format(corpus))\n",
    "  \n",
    "vocab_path=os.path.join(data_root, \"cnn/cnn.vocab\")\n",
    "train_ctf=os.path.join(data_root, \"cnn/training.ctf\")\n",
    "test_ctf=os.path.join(data_root, \"cnn/test.ctf\")\n",
    "validation_ctf=os.path.join(data_root, \"cnn/validation.ctf\")\n",
    "vocab_size=101000\n",
    "if not (file_exists(train_ctf) and file_exists(test_ctf) and file_exists(validation_ctf)):\n",
    "  entity_vocab, word_vocab = Vocabulary.build_vocab(raw_train_data, vocab_path, vocab_size)\n",
    "  Vocabulary.build_corpus(entity_vocab, word_vocab, raw_train_data, train_ctf)\n",
    "  Vocabulary.build_corpus(entity_vocab, word_vocab, raw_test_data, test_ctf)\n",
    "  Vocabulary.build_corpus(entity_vocab, word_vocab, raw_validation_data, validation_ctf)\n",
    "print(\"Training data conversion finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Get GloVe pre-trained embedding data\n",
    "In the paper, they used GloVe pre-trained embedding data for the embedding initialization. The code below is used to download it from the web site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe embedding data downloaded.\n"
     ]
    }
   ],
   "source": [
    "def download_glove_retrained_embedding(target=\".\"):\n",
    "  url=\"http://nlp.stanford.edu/data/glove.6B.zip\"\n",
    "  if os.path.exists(os.path.join(target, \"glove\")):\n",
    "    shutil.rmtree(os.path.join(target, \"glove\"))\n",
    "    \n",
    "  target=os.path.join(target, \"glove\")\n",
    "  if not os.path.exists(target):\n",
    "    os.makedirs(target)\n",
    "  print(\"Start to download GloVe pretrained embedding data from {0} to {1}\".format(url, target))\n",
    "\n",
    "  request = requests.get(url)\n",
    "  zipf = zipfile.ZipFile(io.BytesIO(request.content), mode='r')\n",
    "\n",
    "  zipf.extractall(target)\n",
    "  print(\"Finished to download {0} to {1}\".format(url, target))\n",
    "\n",
    "if not file_exists(os.path.join(data_root, \"glove/glove.6B.300d.txt\")):\n",
    "  download_glove_retrained_embedding(data_root)\n",
    "\n",
    "print(\"GloVe embedding data downloaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Basic CNTK imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import cntk\n",
    "from cntk import Trainer, Axis, device, combine\n",
    "from cntk.layers.blocks import Stabilizer, _initializer_for,  _INFERRED, Parameter, Placeholder, GRU\n",
    "from cntk.layers import Recurrence, Dense\n",
    "from cntk.ops import sequence, reduce_sum, \\\n",
    "    parameter, times, element_times, plus, placeholder, reshape, constant, sigmoid, \\\n",
    "    times_transpose, greater, element_divide, element_select, exp, input\n",
    "from cntk.losses import cosine_distance\n",
    "from cntk.internal import _as_tuple, sanitize_input\n",
    "from cntk.initializer import uniform, glorot_uniform\n",
    "from cntk.io import MinibatchSource, CTFDeserializer, StreamDef, StreamDefs\n",
    "import cntk.ops as ops\n",
    "import cntk.learners as learners\n",
    "# Check for an environment variable defined in CNTK's test infrastructure\n",
    "envvar = 'CNTK_EXTERNAL_TESTDATA_SOURCE_DIRECTORY'\n",
    "def is_test(): return envvar in os.environ\n",
    "\n",
    "# Select the right target device when this notebook is being tested\n",
    "# Currently supported only for GPU \n",
    "\n",
    "if 'TEST_DEVICE' in os.environ:\n",
    "    if os.environ['TEST_DEVICE'] == 'cpu':\n",
    "        raise ValueError('This notebook is currently not support on CPU') \n",
    "    else:\n",
    "        cntk.device.set_default_device(cntk.device.gpu(0))\n",
    "cntk.device.set_default_device(cntk.device.gpu(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Utils\n",
    "Some utils will used during model creation and training stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Logger\n",
    "We use logger to write information both to console and a disk file, so that we can check the inforamtion after the process exited. CNTK has prepackaged loggers as ProgressPrinter and integration with TensorBoard for visualization. However, in this case we share boiler plate code for folks to write ones own logger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import math\n",
    "\n",
    "class logger:\n",
    "  __name=''\n",
    "  __logfile=''\n",
    "\n",
    "  @staticmethod\n",
    "  def init(name=''):\n",
    "    if not os.path.exists(\"model\"):\n",
    "      os.mkdir(\"model\")\n",
    "    if not os.path.exists(\"log\"):\n",
    "      os.mkdir(\"log\")\n",
    "    if name=='' or name is None:\n",
    "      logger.__name='train'\n",
    "    else:\n",
    "      logger.__name=name\n",
    "    logger.__logfile = 'log/{}_{}.log'.format(logger.__name, datetime.now().strftime(\"%m-%d_%H.%M.%S\"))\n",
    "    if os.path.exists(logger.__logfile):\n",
    "      os.remove(logger.__logfile)\n",
    "    print('Log with log file: {0}'.format(logger.__logfile))\n",
    "\n",
    "  @staticmethod\n",
    "  def log(message, toconsole=True):\n",
    "    if logger.__logfile == '' or logger.__logfile is None:\n",
    "      logger.init()\n",
    "    if toconsole:\n",
    "      print(message)\n",
    "    with open(logger.__logfile, 'a', encoding='utf-8') as logf:\n",
    "      logf.write(\"{}| {}\\n\".format(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), message))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## ReasoNet Model\n",
    "\n",
    "Single-turn reasoning models use attention mechanisms with associated deep learning models to emphasize speciﬁc parts of the document which are relevant to the query. However, for many sophisticated comprehension tasks, a human reader often revisits some speciﬁc passage or the question to grasp a better understanding of the problem. Recent work has made use of multiple turns to infer the relation between query, document, and answer. This approach has been demonstrated to produce superior results. We summarize the essence of the [original paper](https://posenhuang.github.io/papers/reasonet_iclr_2017.pdf)  by Shen et. al.  in the remainder of this section.\n",
    "\n",
    "Existing multi-turn models have a ﬁxed number of hops or iterations in their inference, i.e., with predetermined reasoning depth, without regard to the complexity of each individual query or document. However, a human reader may read a document several times and stop when the question in mind has been adequately understood (reaching a certain level of confidence) or terminate after a certain number of tries. \n",
    "\n",
    "ReasoNet tries to mimic the inference process of human readers. With a question in mind, ReasoNet reads a document repeatedly, each time focusing on different parts of the document until a satisfactory answer is found or formed. Moreover, unlike previous approaches using ﬁxed numbers of hops or iterations, ReasoNet introduces a termination state in the inference. This state can decide whether to continue the inference to the next turn after digesting intermediate information, or to terminate the whole inference when it concludes that existing information is sufﬁcient to yield an answer. The number of turns is dynamically modeled by both the document and the query, and will be learned automatically according to the difﬁculty of the problem.\n",
    "\n",
    "![](http://cntk.ai/jup/CNTK_302_reasonet.png)\n",
    "\n",
    "The figure above shows the ResonNet architecture. The model ius made of several components:\n",
    "\n",
    "### VocabSize: \n",
    "\n",
    "For training our ReasoNet, we keep the most frequent|V| = 101k words (not including 584 entities and 1 placeholder marker) in the CNN dataset. \n",
    "\n",
    "### Embedding Layer: \n",
    "\n",
    "We choose word embedding size d = 300, and use the 300 dimensional pretrained GloVe word embeddings (Pennington et al., 2014) for initialization. We also apply dropout with probability 0.2 to the embedding layer. \n",
    "\n",
    "In this implementation we apply a special policy to train the embedding layer. For entities in the context/paragraph, we just use fixed random vectors as the embedding of them and will not update them during training stage. For other words in the context/paragraph and query, we will initialize the embedding using glorot uniform initialization or loading from an existing embedding matrix, e.g. GloVe embedding and they will be updated during training stage. To approch this, we cannot simply adopt existing initializer or embedding lookup funciton in CNTK, and we implimented  `create_random_matrix` and `load_embedding` to create *random initialization matrix* and *load existing embedding matrix*. The class `uniform_initializer` will be used by `load_embedding` to initialize *enities* and other words that cannot be found in the existing embedding matrix (looking up table)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class uniform_initializer:\n",
    "  def __init__(self, scale=1, bias=0, seed=0):\n",
    "    self.seed = seed\n",
    "    self.scale = scale\n",
    "    self.bias = bias\n",
    "    np.random.seed(self.seed)\n",
    "\n",
    "  def reset(self):\n",
    "    np.random.seed(self.seed)\n",
    "\n",
    "  def next(self, size=None):\n",
    "    return np.random.uniform(0, 1, size)*self.scale + self.bias\n",
    "\n",
    "def create_random_matrix(rows, columns):\n",
    "  scale = math.sqrt(6/(rows+columns))*2\n",
    "  rand = uniform_initializer(scale, -scale/2)\n",
    "  embedding = [None]*rows\n",
    "  for i in range(rows):\n",
    "    embedding[i] = np.array(rand.next(columns), dtype=np.float32)\n",
    "  return np.ndarray((rows, columns), dtype=np.float32, buffer=np.array(embedding))\n",
    "\n",
    "def load_embedding(embedding_path, vocab_path, dim, init=None):\n",
    "  entity_vocab, word_vocab = Vocabulary.load_vocab(vocab_path)\n",
    "  vocab_dim = len(entity_vocab) + len(word_vocab) + 1\n",
    "  entity_size = len(entity_vocab)\n",
    "  item_embedding = [None]*vocab_dim\n",
    "  with open(embedding_path, 'r', encoding='utf-8') as embedding:\n",
    "    for line in embedding.readlines():\n",
    "      line = line.strip('\\n')\n",
    "      item = line.split(' ')\n",
    "      if item[0] in word_vocab:\n",
    "        item_embedding[word_vocab[item[0]].id + entity_size + 1] = \\\n",
    "        np.array(item[1:], dtype=\"|S\").astype(np.float32)\n",
    "  if init != None:\n",
    "    init.reset()\n",
    "\n",
    "  for i in range(vocab_dim):\n",
    "    if item_embedding[i] is None:\n",
    "      if init:\n",
    "        item_embedding[i] = np.array(init.next(dim), dtype=np.float32)\n",
    "      else:\n",
    "        item_embedding[i] = np.array([0]*dim, dtype=np.float32)\n",
    "  return np.ndarray((vocab_dim, dim), dtype=np.float32, buffer=np.array(item_embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Bi-GRUEncoder: \n",
    "\n",
    "We apply bi-directional GRU for encoding query and passage into vector representations. We set the number of hidden units to be 256 for the CNN dataset. The recurrent weights of GRUs are initialized with random orthogonal matrices. The other weights in GRU cell are initialized from a uniform distribution between −0.01 and 0.01. We use a shared GRU model for both query and passage. We use the [`GRU`](https://cntk.ai/pythondocs/cntk.layers.html?highlight=gru#cntk.layers.blocks.GRU) block in the Layers library.\n",
    "\n",
    "#### InternalStateController: \n",
    "\n",
    "We choose GRU model as the internal state controller. The number of hidden units in the GRU state controller is 256 for CNN. The initial state of the GRU controller is set to be the last-word of the query representation by a bidirectional-GRU encoder. \n",
    "\n",
    "### Memory and Attention: \n",
    "\n",
    "The memory of the ReasoNet on CNN dataset is composed of query memory and passage memory. $M = (M^{query},M^{doc})$, where $M^{query}$ and $M^{doc}$ are extracted from query bidirectional-GRU encoder and passage bidirectional-GRU encoder respectively. We choose projected cosine similarity function as the attention module. \n",
    "\n",
    "#### Projected Cosine Similarity\n",
    "\n",
    "The *projected cosine similarity* between *internal contoral status* at time step $t$ and  the $i^{th}$ word in the *document* is computed as,\n",
    "\n",
    "$$\n",
    "sim_{t,i}^{doc}= cos\\left(w_1^{doc}m_i^{doc}, w_2^{doc}s_t\\right)\n",
    "$$\n",
    "\n",
    "where $w_1^{doc}$ and $w_2^{doc}$ are project matrices of demension $hidden\\_dim * attention\\_dim$, $m_i^{doc}$ is  the memory vector of the $i^{th}$ word in the document and $s_t$ is the *internal control status* at time step $t$.\n",
    "\n",
    "Similar to the document, the *projected cosine similarity* between *internal contoral status* at time step $t$ and the $i^{th}$ word in the *query* is computed as,\n",
    "\n",
    "$$\n",
    "sim_{t,i}^{query}= cos\\left(w_1^{query}m_i^{query}, w_2^{query}s_t\\right)\n",
    "$$\n",
    "\n",
    "where $w_1^{query}$ and $w_2^{query}$ are project matrices of demension $hidden\\_dim * attention\\_dim$, $m_i^{doc}$ is  the memory vector of the $i^{th}$ word in the query and $s_t$ is the *internal control status* at time step $t$.\n",
    "The *query attention* can be derived similarly.\n",
    "\n",
    "\n",
    "### Attention Score\n",
    "The *attention score* between *internal contoral status* at time step $t$ and the $i^{th}$ word in the *document* is computed as,\n",
    "$$\n",
    "a_{t,i}^{doc}=softmax_{i=1,...,\\left|M^{doc}\\right|}{\\gamma sim_{t,i}^{doc}}\n",
    "$$\n",
    "where, $\\gamma$ is a constant which is set as 10 in the paper to improve numeric stability.\n",
    "\n",
    "The *attention score* between *internal contoral status* at time step $t$ and the $i^{th}$ word in the *query* is computed as,\n",
    "$$\n",
    "a_{t,i}^{query}=softmax_{i=1,...,\\left|M^{query}\\right|}{\\gamma sim_{t,i}^{query}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def project_cosine(project_dim, init = glorot_uniform(), name=''):\n",
    "  \"\"\"\n",
    "  Compute the project cosine similarity of two input sequences, \n",
    "  where each of the input will be projected to a new dimention space (project_dim) via Wi/Wm\n",
    "  \"\"\"\n",
    "  Wi = Parameter(_INFERRED + (project_dim,), init = init, name='Wi')\n",
    "  Wm = Parameter(_INFERRED + (project_dim,), init = init, name='Wm')\n",
    "\n",
    "  status = placeholder(name='status')\n",
    "  memory = placeholder(name='memory')\n",
    "\n",
    "  projected_status = times(status, Wi, name = 'projected_status')   \n",
    "  projected_memory = times(memory, Wm, name = 'projected_memory')\n",
    "  status_br = sequence.broadcast_as(projected_status, projected_memory, name='status_broadcast')\n",
    "  sim = cosine_distance(status_br, projected_memory, name= name)\n",
    "  return sim\n",
    "\n",
    "def attention_score(att_dim, init = glorot_uniform(), name=''):\n",
    "  \"\"\"\n",
    "  Compute the attention score, \n",
    "  where each of the input will be projected to a new dimention space (att_dim) via Wi/Wm\n",
    "  \"\"\"\n",
    "  sim = project_cosine(att_dim, init, name= name+ '_sim')\n",
    "  return sequence.softmax(sim, name = name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Temination gate\n",
    "\n",
    "This gate determines whether to stop further reasoning and execute the Answer module at $t$ time step. Otherwise the ReasoNet will generate an attention vector for the next time step and update the next internal state. We adopt a logistical regression to model the termination variable at each time step and compute the termination probability in each time step is as,\n",
    "$$\n",
    "f_t\\left(s_t;\\theta_t\\right)=sigmoid\\left(w_ts_t+b_t\\right), where\\ \\theta_t=\\left(w_t, b_t\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def termination_gate(init = glorot_uniform(), name=''):\n",
    "  return Dense(1, activation = sigmoid, init=init, name= name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Answer module\n",
    "\n",
    "When the terminate gate output is set to true, the answer module activates. We first compute the *answer attention score* between *internal control status* and each word in the *document*. We treat it as the *probability/score* of each word to be the correct answer. Then sum up all the *probability/score* of the same *entity* in the *document* as the *probability* of the *entity* to be the correct answer at time step $t$ which is the same as ASR. The final answer is an average of all the time steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create Reader\n",
    "\n",
    "The data is stored in CNTK Text Format and we need to create a reader to consume the data. There are 5 columns/streams in the data file, e.g. *context*, *query*, *entity indication*, *label*, *entity ids*. And we use `bind_data` function to bind the *streams* with CNTK functions' (e.g., *model*, *loss*) *arguments* based on their names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_reader(path, vocab_dim, entity_dim, randomize):\n",
    "  \"\"\"\n",
    "  Create data reader for the model\n",
    "  Args:\n",
    "    path: The data path\n",
    "    vocab_dim: The dimention of the vocabulary\n",
    "    entity_dim: The dimention of entities\n",
    "    randomize: Where to shuffle the data before feed into the trainer\n",
    "  \"\"\"\n",
    "  return MinibatchSource(CTFDeserializer(path, StreamDefs(\n",
    "    context  = StreamDef(field='C', shape=vocab_dim, is_sparse=True),\n",
    "    query    = StreamDef(field='Q', shape=vocab_dim, is_sparse=True),\n",
    "    entities  = StreamDef(field='E', shape=1, is_sparse=False),\n",
    "    label   = StreamDef(field='L', shape=1, is_sparse=False),\n",
    "    entity_ids   = StreamDef(field='EID', shape=entity_dim, is_sparse=True)\n",
    "    )), randomize=randomize)\n",
    "\n",
    "def bind_data(func, data):\n",
    "  \"\"\"\n",
    "  Bind data outputs to cntk function arguments based on the argument name\n",
    "  \"\"\"\n",
    "  bind = {}\n",
    "  for arg in func.arguments:\n",
    "    if arg.name == 'query':\n",
    "      bind[arg] = data.streams.query\n",
    "    if arg.name == 'context':\n",
    "      bind[arg] = data.streams.context\n",
    "    if arg.name == 'entity_ids_mask':\n",
    "      bind[arg] = data.streams.entities\n",
    "    if arg.name == 'labels':\n",
    "      bind[arg] = data.streams.label\n",
    "    if arg.name == 'entity_ids':\n",
    "      bind[arg] = data.streams.entity_ids\n",
    "  return bind\n",
    "\n",
    "def get_context_bind_stream(bind):\n",
    "  for key in bind.keys():\n",
    "    if key.name == 'context':\n",
    "      return key\n",
    "  return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create Model\n",
    "\n",
    "#### Model parameters\n",
    "We use `model_params` to wrapper the parameters to create the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class model_params:\n",
    "  def __init__(self, vocab_dim, entity_dim, hidden_dim, embedding_dim=100, embedding_init=None, \n",
    "               share_rnn_param=False, max_rl_steps=5, dropout_rate=None, attention_dim=384, \n",
    "               init=glorot_uniform(), model_name='rsn'):\n",
    "    self.vocab_dim = vocab_dim\n",
    "    self.entity_dim = entity_dim\n",
    "    self.hidden_dim = hidden_dim\n",
    "    self.embedding_dim = embedding_dim\n",
    "    self.embedding_init = embedding_init\n",
    "    self.max_rl_steps = max_rl_steps\n",
    "    self.dropout_rate = dropout_rate\n",
    "    self.init = init\n",
    "    self.model_name = model_name\n",
    "    self.share_rnn_param = share_rnn_param\n",
    "    self.attention_dim = attention_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Attention model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def attention_model(context_memory, query_memory, init_status, hidden_dim, att_dim, \n",
    "                    max_steps = 5, init = glorot_uniform()):\n",
    "  \"\"\"\n",
    "  Create the attention model for reasonet\n",
    "  Args:\n",
    "    context_memory: Context memory\n",
    "    query_memory: Query memory\n",
    "    init_status: Intialize status\n",
    "    hidden_dim: The dimention of hidden state\n",
    "    att_dim: The dimention of attention\n",
    "    max_step: Maxuim number of step to revisit the context memory\n",
    "  \"\"\"\n",
    "\n",
    "  #Instantiate the GRU cell\n",
    "  gru = GRU((hidden_dim*2, ), name='control_status')\n",
    "\n",
    "  status = init_status\n",
    "  output = [None]*max_steps*2\n",
    "  sum_prob = None\n",
    "    \n",
    "  # Compute the context, query and candidate attention scores  \n",
    "  context_attention_score = attention_score(att_dim, name='context_attention')\n",
    "  query_attention_score = attention_score(att_dim, name='query_attention')\n",
    "  answer_attention_score = attention_score(att_dim, name='candidate_attention')\n",
    "    \n",
    "  # Termination gate to compute if one should continue or stop \n",
    "  stop_gate = termination_gate(name='terminate_prob')\n",
    "  prev_stop = 0\n",
    "\n",
    "  # Iterate for maximum number of steps\n",
    "  for step in range(max_steps):\n",
    "    \n",
    "    # Compute the context and query attention weight\n",
    "    context_attention_weight = context_attention_score(status, context_memory)\n",
    "    query_attention_weight = query_attention_score(status, query_memory)\n",
    "    \n",
    "    # Compute the aggretated contribution from context and query; and splice combined attentions\n",
    "    context_attention = sequence.reduce_sum(times(context_attention_weight, context_memory), name='C-Att')\n",
    "    query_attention = sequence.reduce_sum(times(query_attention_weight, query_memory), name='Q-Att')\n",
    "    attention = ops.splice(query_attention, context_attention, name='att-sp')\n",
    "    \n",
    "    # Read the output of the GRU cell\n",
    "    status = gru(status, attention).output\n",
    "    \n",
    "    # Based on the status and the context determine the answer attention\n",
    "    ans_attention = answer_attention_score(status, context_memory)\n",
    "    output[step*2] = ans_attention\n",
    "    \n",
    "    # Based on the output of the GRU cell, evaluate the termination probability\n",
    "    termination_prob = stop_gate(status)\n",
    "\n",
    "    # Compute the probability to stop comprehension\n",
    "    if step < max_steps -1:\n",
    "      stop_prob = prev_stop + ops.log(termination_prob, name='log_stop')\n",
    "    else:\n",
    "      stop_prob = prev_stop\n",
    "    output[step*2+1] = sequence.broadcast_as(ops.exp(stop_prob, name='exp_log_stop'), \n",
    "                                             output[step*2], name='Stop_{0}'.format(step))\n",
    "    prev_stop += ops.log(1-termination_prob, name='log_non_stop')\n",
    "\n",
    "  final_ans = None\n",
    "  for step in range(max_steps):\n",
    "    if final_ans is None:\n",
    "      final_ans = output[step*2] * output[step*2+1]\n",
    "    else:\n",
    "      final_ans += output[step*2] * output[step*2+1]\n",
    "  results = combine(output + [ final_ans ], name='Attention_func')\n",
    "  return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Define  the network\n",
    "#### Dynamic axes in CNTK (Key concept)\n",
    "One of the important concepts in understanding CNTK is the idea of two types of axes:\n",
    "* static axes, which are the traditional axes of a variable's shape, and\n",
    "* dynamic axes, which have dimensions that are unknown until the variable is bound to real data at computation time.\n",
    "\n",
    "The dynamic axes are particularly important in the world of recurrent neural networks. Instead of having to decide a maximum sequence length ahead of time, padding your sequences to that size, and wasting computation, CNTK's dynamic axes allow for variable sequence lengths that are automatically packed in minibatches to be as efficient as possible.\n",
    "\n",
    "When setting up sequences, there are two dynamic axes that are important to consider. The first is the batch axis, which is the axis along which multiple sequences are batched. The second is the dynamic axis particular to that sequence. The latter is specific to a particular input because of variable sequence lengths in your data. In CNTK, we use the dynamic axe name to idenitify different dynamic axes, and all sequence oprations between different variables require them have the same dynamic axes which means they must have the same length on all the axes. \n",
    "\n",
    "In ReasoNet networks, we have five input streams/sequences: *query*, *paragraph*, *label*, *entity id*, *entity indicator*, where *entity id* and *entity indicator* are helper sequences. *Query*, *paragraph* and *entity id* have different sequence lengths so they have different sequence dynamic axis. *label* and *entity id* have the same sequence length as *paragraph*, so they share the same dynamic axis. As a result, there will be two sequence_axes, *sourceAxis* and *contextAxis* when we create the model. The we can define input *query_sequence* over *sourceAxis*, *context_sequence* and *entity_ids_mask* over *contextAxis*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_model(params):\n",
    "  \"\"\"\n",
    "  Create ReasoNet model\n",
    "  Args:\n",
    "    params (class:`model_params`): The parameters used to create the model\n",
    "  \"\"\"\n",
    "  logger.log(\"Create model: dropout_rate: {0}, init:{1}, embedding_init: {2}\"\\\n",
    "             .format(params.dropout_rate, params.init, params.embedding_init))\n",
    "    \n",
    "  # Query and Doc/Context/Paragraph inputs to the model\n",
    "  query_seq_axis = Axis('sourceAxis')\n",
    "  context_seq_axis = Axis('contextAxis')\n",
    "\n",
    "  # Specify the query and the context sequence inputs\n",
    "  query_sequence = sequence.input(shape=(params.vocab_dim), is_sparse=True, sequence_axis=query_seq_axis, name='query')\n",
    "  context_sequence = sequence.input(shape=(params.vocab_dim), is_sparse=True, sequence_axis=context_seq_axis, name='context')\n",
    "  \n",
    "  # Specify the container to hold entity ids mask with the same length as context sequence \n",
    "  # where each iterm is and indicator of whether the corresponding word in the context is an entity or not.\n",
    "  entity_ids_mask = sequence.input(shape=(1,), is_sparse=False, sequence_axis=context_seq_axis, name='entity_ids_mask')\n",
    "    \n",
    "  # Either random initialize the embedding or load a pre-dertermined embedding (say GLOVE)  \n",
    "  if params.embedding_init is None:\n",
    "    embedding_init = create_random_matrix(params.vocab_dim, params.embedding_dim)\n",
    "  else:\n",
    "    embedding_init = params.embedding_init\n",
    "  embedding = parameter(shape=(params.vocab_dim, params.embedding_dim), init=None)\n",
    "  embedding.value = embedding_init\n",
    "  constant_embedding = constant(embedding_init, shape=(params.vocab_dim, params.embedding_dim))\n",
    "\n",
    "  # Optionally add dropouts in the embeddings\n",
    "  if params.dropout_rate is not None:\n",
    "    query_embedding  = ops.dropout(times(query_sequence , embedding), params.dropout_rate, \n",
    "                                   name='query_embedding')\n",
    "    context_embedding = ops.dropout(times(context_sequence, embedding), params.dropout_rate, \n",
    "                                    name='context_embedding')\n",
    "  else:\n",
    "    query_embedding  = times(query_sequence , embedding, name='query_embedding')\n",
    "    context_embedding = times(context_sequence, embedding, name='context_embedding')\n",
    "    \n",
    "  \n",
    "  # Create containers for the GRU weights for query and context \n",
    "  context_gru_weights = Parameter(_INFERRED + (params.hidden_dim,), init=glorot_uniform(), name='context_gru_params')\n",
    "  if params.share_rnn_param:\n",
    "    query_gru_weights = context_gru_weights\n",
    "  else:\n",
    "    query_gru_weights = Parameter(_INFERRED + (params.hidden_dim,), init=glorot_uniform(), name='query_gru_params')\n",
    "\n",
    "  # We use constant random vectors as the embedding of entities in the paragraph, \n",
    "  # as we treat them as meaningless symbolic in the paragraph which is equal to entity shuffle\n",
    "  entity_embedding = ops.times(context_sequence, constant_embedding, name='constant_entity_embedding')\n",
    "  \n",
    "  # Unlike other words in the context, \n",
    "  # we keep the entity vectors fixed as a random vector so that each vector just means an identifier \n",
    "  # of different entities in the context and it has no semantic meaning\n",
    "  full_context_embedding = ops.element_select(entity_ids_mask, entity_embedding, context_embedding)\n",
    "  context_memory = ops.optimized_rnnstack(full_context_embedding, context_gru_weights, params.hidden_dim, 1, \n",
    "                                          True, recurrent_op='gru', name='context_mem')\n",
    "\n",
    "  query_memory = ops.optimized_rnnstack(query_embedding, query_gru_weights, params.hidden_dim, 1, True, \n",
    "                                        recurrent_op='gru', name='query_mem')\n",
    "    \n",
    "  qfwd = ops.slice(sequence.last(query_memory), -1, 0, params.hidden_dim, name='fwd')\n",
    "  qbwd = ops.slice(sequence.first(query_memory), -1, params.hidden_dim, params.hidden_dim*2, name='bwd')\n",
    "  init_status = ops.splice(qfwd, qbwd, name='Init_Status') # get last fwd status and first bwd status\n",
    "\n",
    "  return attention_model(context_memory, query_memory, init_status, params.hidden_dim, \n",
    "                         params.attention_dim, max_steps = params.max_rl_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Loss function (Contrastive Reward)\n",
    "\n",
    "One of the signiﬁcant challenges ReasoNet faces is how to design an efﬁcient training method, since the termination state is discrete and not connected to the ﬁnal output. This prohibits the canonical back-propagation method from being directly applied to train ReasoNet. This challenge is tackled by a novel deep reinforcement learning method called Contrastive Reward (CR) to successfully train ReasoNet.\n",
    "\n",
    "Unlike traditional reinforcement learning optimization methods using a global variable to capture rewards, CR utilizes an instance-based reward baseline assignment. Experiments show the superiority of CR in both training speed and accuracy. \n",
    "\n",
    "Finally, by accounting for a dynamic termination state during inference and applying the proposed deep reinforcement learning optimization method, ReasoNet can achieve the state-of-the-art results in machine comprehension datasets, including unstructured CNN and Daily Mail datasets, and a proposed structured Graph Reachability dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def contrastive_reward(labels, predictions_and_stop_probabilities):\n",
    "  \"\"\"\n",
    "  Compute the contrastive reward loss in paper 'ReasoNet: \n",
    "    Learning to Stop Reading in Machine Comprehension'\n",
    "  Args:\n",
    "    labels: The lables\n",
    "    predictions_and_stop_probabilities: A list of tuples, \n",
    "    each tuple contains the prediction and stop probability of the coresponding step.\n",
    "  \"\"\"\n",
    "  base = None\n",
    "  avg_rewards = None\n",
    "  for step in range(len(predictions_and_stop_probabilities)):\n",
    "    pred = predictions_and_stop_probabilities[step][0]\n",
    "    stop = predictions_and_stop_probabilities[step][1]\n",
    "    if base is None:\n",
    "      base = ops.element_times(pred, stop)\n",
    "    else:\n",
    "      base = ops.plus(ops.element_times(pred, stop), base)\n",
    "    \n",
    "  avg_rewards = ops.stop_gradient(sequence.reduce_sum(base*labels))\n",
    "  base_reward = sequence.broadcast_as(avg_rewards, base, name = 'base_line')\n",
    "    \n",
    "  # While  the learner will mimize the loss by default, we want it to maximize the rewards\n",
    "  # Maximum rewards => minimal -rewards\n",
    "  # So we use (1-r/b) as the rewards instead of (r/b-1)\n",
    "  step_cr = ops.stop_gradient(1- ops.element_divide(labels, base_reward))\n",
    "  normalized_contrastive_rewards = ops.element_times(base, step_cr)\n",
    "  rewards = sequence.reduce_sum(normalized_contrastive_rewards) + avg_rewards\n",
    "  return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Loss and accuracy\n",
    "\n",
    "At loss computation stage, we will consume another two inputs, *enity_ids* and *labels*. They have the same length and share the same dynamic axes which is created programatically (see the comments inline the code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def accuracy_func(prediction, label, name='accuracy'):\n",
    "  \"\"\"\n",
    "  Compute the accuracy of the prediction\n",
    "  \"\"\"\n",
    "  pred_max = ops.hardmax(prediction, name='pred_max')\n",
    "  norm_label = ops.equal(label, [1], name='norm_label')\n",
    "  acc = ops.times_transpose(pred_max, norm_label, name='accuracy')\n",
    "  return acc\n",
    "\n",
    "def loss(model, params):\n",
    "  \"\"\"\n",
    "  Compute the loss and accuracy of the model output\n",
    "  \"\"\"\n",
    "  model_args = {arg.name:arg for arg in model.arguments}\n",
    "  context = model_args['context']\n",
    "  entity_ids_mask = model_args['entity_ids_mask']\n",
    "  entity_condition = greater(entity_ids_mask, 0, name='condidion')\n",
    "\n",
    "  # Get all the enities in the paragraph via gather operator, which will create a new dynamic sequence axis \n",
    "  entities_all = sequence.gather(entity_condition, entity_condition, name='entities_all')\n",
    "\n",
    "  # The generated dynamic axis has the same length as the input enity id sequence, \n",
    "  # so we asign it as the entity id's dynamic axis.\n",
    "  entity_ids = input(shape=(params.entity_dim), is_sparse=True, \n",
    "                              dynamic_axes=entities_all.dynamic_axes, name='entity_ids')\n",
    "    \n",
    "  wordvocab_dim = params.vocab_dim\n",
    "  labels_raw = input(shape=(1,), is_sparse=False, dynamic_axes=context.dynamic_axes, \n",
    "                              name='labels')\n",
    "    \n",
    "  answers = sequence.scatter(sequence.gather(model.outputs[-1], entity_condition), entities_all, name='Final_Ans')\n",
    "  labels = sequence.scatter(sequence.gather(labels_raw, entity_condition), entities_all, name='EntityLabels')\n",
    "  entity_id_matrix = ops.reshape(entity_ids, params.entity_dim)\n",
    "  \n",
    "  # We aggragate the scores of the same enities at different position in \n",
    "  # the paragraph as the final score of the entity\n",
    "  aggregated_pred = sequence.reduce_sum(element_times(answers, entity_id_matrix))\n",
    "  aggregated_label = ops.greater_equal(sequence.reduce_sum(element_times(labels, entity_id_matrix)), 1)\n",
    "  predictions_and_stop_probabilities=[]\n",
    "\n",
    "  for step in range(int((len(model.outputs)-1)/2)):\n",
    "    predictions_and_stop_probabilities += [(model.outputs[step*2], model.outputs[step*2+1])]\n",
    "  loss_value = contrastive_reward(labels_raw, predictions_and_stop_probabilities)\n",
    "  accuracy = accuracy_func(aggregated_pred, aggregated_label, name='accuracy')\n",
    "  apply_loss = combine([loss_value, answers, labels, accuracy], name='Loss')\n",
    "  return apply_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Adam Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_adam_learner(learn_params, learning_rate = 0.0005, gradient_clipping_threshold_per_sample=0.001):\n",
    "  \"\"\"\n",
    "  Create adam learner\n",
    "  \"\"\"\n",
    "  lr_schedule = learners.learning_rate_schedule(learning_rate, learners.UnitType.sample)\n",
    "  momentum = learners.momentum_schedule(0.90)\n",
    "  gradient_clipping_threshold_per_sample = gradient_clipping_threshold_per_sample\n",
    "  gradient_clipping_with_truncation = True\n",
    "  momentum_var = learners.momentum_schedule(0.999)\n",
    "  lr = learners.adam(learn_params, lr_schedule, momentum, True, momentum_var,\n",
    "          gradient_clipping_threshold_per_sample = gradient_clipping_threshold_per_sample,\n",
    "          gradient_clipping_with_truncation = gradient_clipping_with_truncation)\n",
    "  learner_desc = 'Alg: Adam, learning rage: {0}, momentum: {1}, gradient clip: {2}'\\\n",
    "    .format(learning_rate, momentum[0], gradient_clipping_threshold_per_sample)\n",
    "  logger.log(\"Create learner. {0}\".format(learner_desc))\n",
    "  return lr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def __evaluation(trainer, data, bind, minibatch_size, epoch_size):\n",
    "  \"\"\"\n",
    "  Evaluate the loss and accurate of the evaluation data set during training stage\n",
    "  \"\"\"\n",
    "  if epoch_size is None:\n",
    "    epoch_size = 1\n",
    "  context_stream = get_context_bind_stream(bind)\n",
    "  eval_acc = 0\n",
    "  eval_s = 0\n",
    "  k = 0\n",
    "  print(\"Start evaluation with {0} samples ...\".format(epoch_size))\n",
    "  while k < epoch_size:\n",
    "    mbs = min(epoch_size - k, minibatch_size)\n",
    "    mb = data.next_minibatch(mbs, input_map=bind)\n",
    "    k += mb[context_stream].num_samples\n",
    "    sm = mb[context_stream].num_sequences\n",
    "    avg_acc = trainer.test_minibatch(mb)\n",
    "    eval_acc += sm*avg_acc\n",
    "    eval_s += sm\n",
    "    sys.stdout.write('.')\n",
    "    sys.stdout.flush()\n",
    "  eval_acc /= eval_s\n",
    "  print(\"\")\n",
    "  logger.log(\"Evaluation Acc: {0}, samples: {1}\".format(eval_acc, eval_s))\n",
    "  return eval_acc\n",
    "\n",
    "def train(model, m_params, learner, train_data, max_epochs=1, \n",
    "          save_model_flag=False, epoch_size=270000, eval_data=None, eval_size=None, \n",
    "          check_point_freq=0.1, minibatch_size=50000, model_name='rsn'):\n",
    "  \"\"\"\n",
    "  Train the model\n",
    "  Args:\n",
    "    model: The created model\n",
    "    m_params: Model parameters\n",
    "    learner: The learner used to train the model\n",
    "  \"\"\"\n",
    "  criterion_loss = loss(model, m_params)\n",
    "  loss_func = criterion_loss.outputs[0]\n",
    "  eval_func = criterion_loss.outputs[-1]\n",
    "  trainer = Trainer(model.outputs[-1], (loss_func, eval_func), learner)\n",
    "\n",
    "  # Get minibatches of sequences to train with and perform model training\n",
    "  # bind inputs to data from readers\n",
    "  train_bind = bind_data(criterion_loss, train_data)\n",
    "  context_stream = get_context_bind_stream(train_bind)\n",
    "  eval_bind = bind_data(criterion_loss, eval_data)\n",
    "\n",
    "  i = 0\n",
    "  minibatch_count = 0\n",
    "  training_progress_output_freq = 500\n",
    "  check_point_interval = int(epoch_size*check_point_freq)\n",
    "  check_point_id = 0\n",
    "  for epoch in range(max_epochs):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_samples = 0\n",
    "    i = 0\n",
    "    win_loss = 0\n",
    "    win_acc = 0\n",
    "    win_samples = 0\n",
    "    chk_loss = 0\n",
    "    chk_acc = 0\n",
    "    chk_samples = 0\n",
    "    while i < epoch_size:\n",
    "      # get next minibatch of training data\n",
    "      mbs = min(minibatch_size, epoch_size - i)\n",
    "      mb_train = train_data.next_minibatch(minibatch_size, input_map=train_bind)\n",
    "      i += mb_train[context_stream].num_samples\n",
    "      trainer.train_minibatch(mb_train)\n",
    "      minibatch_count += 1\n",
    "      sys.stdout.write('.')\n",
    "      sys.stdout.flush()\n",
    "      # collect epoch-wide stats\n",
    "      samples = trainer.previous_minibatch_sample_count\n",
    "      ls = trainer.previous_minibatch_loss_average * samples\n",
    "      acc = trainer.previous_minibatch_evaluation_average * samples\n",
    "      epoch_loss += ls\n",
    "      epoch_acc += acc\n",
    "      win_loss += ls\n",
    "      win_acc += acc\n",
    "      chk_loss += ls\n",
    "      chk_acc += acc\n",
    "      epoch_samples += samples\n",
    "      win_samples += samples\n",
    "      chk_samples += samples\n",
    "      if int(epoch_samples/training_progress_output_freq) != \\\n",
    "        int((epoch_samples-samples)/training_progress_output_freq):\n",
    "        print('')\n",
    "        logger.log(\"Lastest sample count = {}, Train Loss: {}, Evalualtion ACC: {}\"\\\n",
    "                   .format(win_samples, win_loss/win_samples,\n",
    "          win_acc/win_samples))\n",
    "        logger.log(\"Total sample count = {}, Train Loss: {}, Evalualtion ACC: {}\"\\\n",
    "                   .format(chk_samples, chk_loss/chk_samples,\n",
    "          chk_acc/chk_samples))\n",
    "        win_samples = 0\n",
    "        win_loss = 0\n",
    "        win_acc = 0\n",
    "      new_chk_id = int(i/check_point_interval)\n",
    "      if new_chk_id != check_point_id and i < epoch_size :\n",
    "        check_point_id = new_chk_id\n",
    "        print('')\n",
    "        logger.log(\"--- CHECKPOINT %d: samples=%d, loss = %.2f, acc = %.2f%% ---\" % (check_point_id, \n",
    "                                                                                     chk_samples, \n",
    "                                                                                     chk_loss/chk_samples, \n",
    "                                                                                     100.0*(chk_acc/chk_samples)))\n",
    "        if eval_data:\n",
    "          __evaluation(trainer, eval_data, eval_bind, minibatch_size, eval_size)\n",
    "        if save_model_flag:\n",
    "          # save the model every epoch\n",
    "          model_filename = os.path.join('model', \"model_%s_%02d_%03d.dnn\" % (model_name, epoch, check_point_id))\n",
    "          model.save_model(model_filename)\n",
    "          logger.log(\"Saved model to '%s'\" % model_filename)\n",
    "        chk_samples = 0\n",
    "        chk_loss = 0\n",
    "        chk_acc = 0\n",
    "\n",
    "    print('')\n",
    "    logger.log(\"--- EPOCH %d: samples=%d, loss = %.2f, acc = %.2f%% ---\" % (epoch, epoch_samples,\n",
    "                                                                            epoch_loss/epoch_samples,\n",
    "                                                                            100.0*(epoch_acc/epoch_samples)))\n",
    "  eval_acc = 0\n",
    "  if eval_data:\n",
    "    eval_acc = __evaluation(trainer, eval_data, eval_bind, minibatch_size, eval_size)\n",
    "  if save_model_flag:\n",
    "    # save the model every epoch\n",
    "    model_filename = os.path.join('model', \"model_%s_final.dnn\" % (model_name))\n",
    "    model.save_model(model_filename)\n",
    "    logger.log(\"Saved model to '%s'\" % model_filename)\n",
    "  return (epoch_loss/epoch_samples, epoch_acc/epoch_samples, eval_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train the model for CNN data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log with log file: log/cnn_model_03-30_01.41.18.log\n",
      "Create model: dropout_rate: 0.2, init:<cntk.cntk_py.Dictionary; proxy of <Swig Object of type 'CNTK::ImageTransform *' at 0x7fda3b1f1e10> >, embedding_init: None\n",
      "Create learner. Alg: Adam, learning rage: 0.0005, momentum: 0.9, gradient clip: 0.001\n",
      "....\n",
      "--- CHECKPOINT 1: samples=5, loss = 0.01, acc = 40.00% ---\n",
      "Start evaluation with 2993 samples ...\n",
      "....\n",
      "Evaluation Acc: 0.25, samples: 4\n",
      "....\n",
      "--- CHECKPOINT 2: samples=4, loss = 0.01, acc = 25.00% ---\n",
      "Start evaluation with 2993 samples ...\n",
      "....\n",
      "Evaluation Acc: 0.5, samples: 4\n",
      "...\n",
      "--- CHECKPOINT 3: samples=4, loss = 0.01, acc = 0.00% ---\n",
      "Start evaluation with 2993 samples ...\n",
      "....\n",
      "Evaluation Acc: 0.0, samples: 4\n",
      "..\n",
      "--- CHECKPOINT 4: samples=2, loss = 0.01, acc = 0.00% ---\n",
      "Start evaluation with 2993 samples ...\n",
      "...\n",
      "Evaluation Acc: 0.3333333333333333, samples: 3\n",
      "...\n",
      "--- CHECKPOINT 5: samples=3, loss = 0.00, acc = 0.00% ---\n",
      "Start evaluation with 2993 samples ...\n",
      "....\n",
      "Evaluation Acc: 0.16666666666666666, samples: 6\n",
      "..\n",
      "--- CHECKPOINT 6: samples=2, loss = 0.00, acc = 0.00% ---\n",
      "Start evaluation with 2993 samples ...\n",
      ".....\n",
      "Evaluation Acc: 0.16666666666666666, samples: 6\n",
      "....\n",
      "--- CHECKPOINT 7: samples=4, loss = 0.01, acc = 50.00% ---\n",
      "Start evaluation with 2993 samples ...\n",
      "...\n",
      "Evaluation Acc: 0.25, samples: 4\n",
      "...\n",
      "--- CHECKPOINT 8: samples=4, loss = 0.02, acc = 50.00% ---\n",
      "Start evaluation with 2993 samples ...\n",
      "....\n",
      "Evaluation Acc: 0.25, samples: 4\n",
      "...\n",
      "--- CHECKPOINT 9: samples=3, loss = 0.01, acc = 0.00% ---\n",
      "Start evaluation with 2993 samples ...\n",
      "....\n",
      "Evaluation Acc: 0.2, samples: 5\n",
      "..\n",
      "--- EPOCH 0: samples=34, loss = 0.01, acc = 23.53% ---\n",
      "Start evaluation with 2993 samples ...\n",
      "....\n",
      "Evaluation Acc: 0.75, samples: 4\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import cntk.device as device\n",
    "import numpy as np\n",
    "from cntk.ops.tests.ops_test_utils import cntk_device\n",
    "from cntk.ops import input_variable, past_value, future_value\n",
    "from cntk.io import MinibatchSource\n",
    "from cntk import Trainer, Axis, device, combine\n",
    "from cntk.layers import Recurrence, Convolution\n",
    "import cntk.ops as ops\n",
    "import cntk\n",
    "import math\n",
    "\n",
    "def train_reasonet_cnn():\n",
    "  logger.init(\"cnn_model\")\n",
    "  data_path = train_ctf\n",
    "  eval_path = validation_ctf\n",
    "  vocab_dim = 101585\n",
    "  entity_dim = 586\n",
    "  epoch_size=289716292\n",
    "  eval_size=2993016\n",
    "  hidden_dim=256\n",
    "  max_rl_steps=5\n",
    "  max_epochs=5\n",
    "  embedding_dim=300\n",
    "  att_dim = 384\n",
    "  # The average sequence length is about 700, so we set the minibatch_size to 50000 in sequence num,\n",
    "  # which is about 70 samples/instanes per minibatch\n",
    "  minibatch_size=50000 \n",
    "  train_data = create_reader(data_path, vocab_dim, entity_dim, True)\n",
    "  eval_data = create_reader(eval_path, vocab_dim, entity_dim, False) \\\n",
    "    if eval_path is not None else None\n",
    "    \n",
    "  glove_path = os.path.join(data_root, \"glove/glove.6B.{0}d.txt\".format(embedding_dim))  \n",
    "  scale = math.sqrt(6/(vocab_dim+embedding_dim))*2\n",
    "  init = uniform_initializer(scale, -scale/2)\n",
    "  embedding_init = load_embedding(glove_path, vocab_path, embedding_dim, init) if os.path.exists(glove_path) \\\n",
    "    else None\n",
    "    \n",
    "  demo_only=True\n",
    "  if demo_only:\n",
    "    # Use a smaller minibatch_size to reduce memory usage for demo popurse only\n",
    "    minibatch_size=1000\n",
    "    hidden_dim = 128\n",
    "    att_dim = 128\n",
    "    max_rl_steps = 2\n",
    "    max_epochs = 1\n",
    "    embedding_dim=100\n",
    "    embedding_init = None\n",
    "    epoch_size=28971\n",
    "    eval_size=2993\n",
    "    \n",
    "  params = model_params(vocab_dim = vocab_dim, entity_dim = entity_dim, hidden_dim = hidden_dim, \n",
    "                        embedding_dim = embedding_dim, attention_dim=att_dim, max_rl_steps = max_rl_steps,\n",
    "                        embedding_init = embedding_init, dropout_rate = 0.2, share_rnn_param=True)\n",
    "\n",
    "  model = create_model(params)\n",
    "  learner = create_adam_learner(model.parameters)\n",
    "  (train_loss, train_acc, eval_acc) = train(model, params, learner, train_data, \n",
    "                                            max_epochs=max_epochs, epoch_size=epoch_size, \n",
    "                                            save_model_flag=False, model_name=os.path.basename(data_path),\n",
    "                                            eval_data=eval_data, eval_size=eval_size, check_point_freq=0.1,\n",
    "                                            minibatch_size = minibatch_size)\n",
    "\n",
    "train_reasonet_cnn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Test the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log with log file: log/cnn_test_03-30_01.43.19.log\n",
      "...............................................\n",
      "Evaluation acc: 0.7207629768605378, loss: 0.5853168244209791, samples: 3198 in 148.42999999970198 seconds\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import cntk.device as device\n",
    "import numpy as np\n",
    "import math\n",
    "from cntk import load_model\n",
    "\n",
    "def test_cnn_model(model_path):\n",
    "  logger.init(\"cnn_test\")\n",
    "  test_path = os.path.join(data_root, \"cnn/test.ctf\")\n",
    "  test_size=2291183\n",
    "  vocab_dim = 101585\n",
    "  entity_dim = 586\n",
    "  hidden_dim=256\n",
    "  max_rl_steps=5\n",
    "  embedding_dim=300\n",
    "  att_dim = 384\n",
    "  minibatch_size=50000\n",
    "  share_rnn = True\n",
    "\n",
    "  test_data = create_reader(test_path, vocab_dim, entity_dim, False)\n",
    "  embedding_init = None\n",
    "\n",
    "  params = model_params(vocab_dim = vocab_dim, entity_dim = entity_dim, hidden_dim = hidden_dim,\n",
    "                        embedding_dim = embedding_dim, attention_dim=att_dim, max_rl_steps = max_rl_steps,\n",
    "                        embedding_init = embedding_init, dropout_rate = 0.2, share_rnn_param = share_rnn)\n",
    "\n",
    "  model = load_model(model_path)\n",
    "  loss_func = loss(model, params)\n",
    "  bind = bind_data(loss_func, test_data)\n",
    "  context_stream = get_context_bind_stream(bind)\n",
    "  loss_sum = 0\n",
    "  acc_sum = 0\n",
    "  samples_sum = 0\n",
    "  i = 0\n",
    "  start = os.times()\n",
    "  while i<test_size:\n",
    "    mbs = min(test_size - i, minibatch_size)\n",
    "    mb = test_data.next_minibatch(mbs, bind)\n",
    "    outs = loss_func.eval(mb)\n",
    "    loss_value = np.sum(outs[loss_func.outputs[0]])\n",
    "    acc = np.sum(outs[loss_func.outputs[-1]])\n",
    "    i += mb[context_stream].num_samples\n",
    "    samples = mb[context_stream].num_sequences\n",
    "    samples_sum += samples\n",
    "    acc_sum += acc\n",
    "    loss_sum += loss_value\n",
    "    sys.stdout.write('.')\n",
    "    sys.stdout.flush()\n",
    "  end = os.times()\n",
    "  total = end.elapsed - start.elapsed\n",
    "  print(\"\")\n",
    "  print(\"Evaluation acc: {0}, loss: {1}, samples: {2} in {3} seconds\".format(acc_sum/samples_sum, loss_sum/samples_sum, samples_sum, total))\n",
    "\n",
    "test_cnn_model(os.path.join(data_root, \"../model/model_training.ctf_001.dnn\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def predict(model, params):\n",
    "  \"\"\"\n",
    "  Compute the prediction result of the given model\n",
    "  \"\"\"\n",
    "  model_args = {arg.name:arg for arg in model.arguments}\n",
    "  context = model_args['context']\n",
    "  entity_ids_mask = model_args['entity_ids_mask']\n",
    "  entity_condition = greater(entity_ids_mask, 0, name='condidion')\n",
    "\n",
    "  # Get all the enities in the paragraph via gather operator, which will create a new dynamic sequence axis\n",
    "  entities_all = sequence.gather(entity_condition, entity_condition, name='entities_all')\n",
    "\n",
    "  # The generated dynamic axis has the same length as the input enity id sequence,\n",
    "  # so we asign it as the entity id's dynamic axis.\n",
    "  entity_ids = input(shape=(params.entity_dim), is_sparse=True,\n",
    "                              dynamic_axes=entities_all.dynamic_axes, name='entity_ids')\n",
    "    \n",
    "  wordvocab_dim = params.vocab_dim\n",
    "  answers = sequence.scatter(sequence.gather(model.outputs[-1], entity_condition), entities_all, name='Final_Ans')\n",
    "    \n",
    "  entity_id_matrix = ops.slice(ops.reshape(entity_ids, params.entity_dim), -1, 1, params.entity_dim)\n",
    "  expand_pred = sequence.reduce_sum(element_times(answers, entity_id_matrix))\n",
    "  pred_max = ops.hardmax(expand_pred, name='pred_max')\n",
    "  return pred_max\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log with log file: log/cnn_test_03-30_01.45.48.log\n",
      ".....\n",
      "Evaluated samples: 5 in 2.530000001192093 seconds\n",
      "===============\n",
      "[0] Doc: @entity3 ( @entity2 ) @entity1 heavyweight boxing champion @entity0 has an important title defense coming up , but his thoughts continue to be dominated by the ongoing fight for democracy in @entity8 . speaking to @entity2 from his @entity3 training base ahead of the april 25 showdown with @entity12 challenger @entity11 in @entity13 , @entity0 said the crisis in his homeland has left him shocked and upset . \" my country is unfortunately suffering in the war with @entity18 -- not that @entity8 tried to give any aggression to any other nation , in this particular case @entity18 , unfortunately it 's the other way around , \" @entity0 told @entity2 . \" i never thought that our brother folk is going to have war with us , so that @entity8 and @entity18 are going to be divided with blood , \" he added . \" unfortunately , we do n't know how far it 's going to go and how worse it 's going to get . the aggression , in the military presence of ( @entity18 ) soldiers and military equipment in my country , @entity8 , is upsetting . \" @entity0 is the reigning @entity33 , @entity34 , @entity35 and @entity36 champion and has , alongside older brother @entity37 , dominated the heavyweight division in the 21st century . @entity37 , who retired from boxing in 2013 , is a prominent figure in @entity8 politics . the 43 - year - old has led the @entity43 since 2010 and was elected mayor of @entity45 in may last year . tensions in the former @entity48 state remain high despite a ceasefire agreed in february as @entity50 , led by @entity52 chancellor @entity51 and president of france @entity53 , tries to broker a peace deal between the two sides . the crisis in @entity8 began in november 2013 when former president @entity58 scuttled a trade deal with the @entity60 in favor of forging closer economic ties with @entity18 . the move triggered a wave of anti-government protests which came to a head @entity45 's @entity67 in february 2014 when clashes between protesters and government security forces left around 100 dead . the following month , @entity18 troops entered @entity8 's @entity74 peninsula before @entity18 president @entity75 completed the annexation of @entity74 -- a move denounced by most of the world as illegitimate -- after citizens of the region had voted in favor of leaving @entity8 in a referendum . more than 5,000 people have been killed in the conflict to date . \" people are dying in @entity8 every single day , \" @entity0 said . \" i do not want to see it , nobody wants to see it ... it 's hard to believe these days something like that in @entity50 -- and @entity8 is @entity50 -- can happen . \" but with the backing of the international community , @entity0 is confident @entity8 can forge a democratic future rather than slide back towards a @entity48 - era style dictatorship . \" i really wish and want this conflict to be solved and it can only be solved with @entity98 help , \" he said . \" @entity8 is looking forward to becoming a democratic country and live under @entity98 democracy . this is our decision and this is our will to get what we want . \" if somebody wants to try to put ( us ) back to the @entity48 times and be part of the @entity108 , we disagree with that . we want to be in freedom . \" we have achieved many things in moving forward and showed to the world that we do not want to live under a dictatorship . \" @entity0 , whose comments were made as part of a wide - ranging interview for @entity2 's @entity118 series , is routinely kept abreast of developments in @entity8 by brother @entity37 but also returns home whenever he can . \" as much time as i can spend , i am there in the @entity8 . it 's not like i am getting the news from mass media and making my own adjustments and judgments on what 's going on . it 's an actual presence and understanding from the inside ... it obviously affects my life , it affects the life of my family . \" the 39 - year - old and his fiancée @entity137 celebrated happier times last december when the @entity12 actress gave birth to a baby daughter , @entity142 . \" i need to get used to it that i 'm a father , which is really exciting . i hope i 'm going to have a big family with multiple kids , \" he said . @entity0 is n't sure when he 'll finally hang up his gloves . \" i do n't know how long i can last ... motivation and health have to be there to continue . \" but after leaving almost all his boxing opponents battered and bruised -- the @entity8 is seeking an impressive 18th consecutive title defense against @entity11 -- @entity0 is keen to carry on fighting his own and his country 's corner in the opposite way outside the ring . \" i just really want that we 'll have less violence in the world ... i hope in peace we can do anything , but if we have war then it 's definitely going to leave us dull and numb . \" watch @entity0 's @entity118 interview on @entity2 's @entity165 on wednesday april 8 at 1130 , 1245 , 1445 , 2130 , 2245 and 2345 and thursday april 9 at 0445 ( all times gmt ) and here online .\n",
      " Query: @placeholder faces @entity12 challenger @entity11 in @entity13 on april 25\n",
      " Answer: @entity0\n",
      " Expected: @entity0\n",
      "===============\n",
      "[1] Doc: ( @entity0 ) it was a masterful performance . i watched in awe on thursday as cardiothoracic surgeon and celebrity talk show host @entity4 surfed a gargantuan wave of criticism to shore . i should have expected that dr. @entity4 would keep standing in the face of charges from a group of colleagues that he pushed \" quack treatments ... for personal financial gain . \" and thanks to the ineptitude of his critics this round , he may have actually boosted his media empire . but he 's increasingly serving himself at a cost to @entity24 and @entity25 . if dr. @entity4 was ever going to go down , surely his ship would 've sunk last summer in the wake of his disastrous testimony before a @entity30 subcommittee . he was ostensibly invited to speak as an expert witness about bogus weight loss products , but sen. @entity33 , chairwoman of the @entity30 's @entity34 , instead made him her chief example of the kind of snake oil salesmen that keep hoodwinking consumers into thinking there 's a quick fix for their expanding waistlines . @entity43 subsequently eviscerated @entity4 on his hit @entity45 show \" @entity46 . \" in a segment that 's now garnered over 6 million @entity49 views , @entity43 makes quick work of @entity4 's claims about \" magic \" green coffee beans , a product that 's now earned a $ 9 million @entity55 fine for its false marketing claims . despite his utter humiliation , dr. @entity4 soldiered on , with his university and hospital continuing to stand by him , and with @entity61 and @entity62 , who co-produce \" the dr. @entity64 , \" fully behind his program . if the @entity30 could n't bring him down , what made this particular collection of 10 doctors think they could do it with their recent letter to @entity24 , where dr. @entity4 holds a tenured professorship and administrative position in the @entity74 and performs his duties at @entity24 - affiliated @entity25 ? the doctors insisted that the university must disassociate itself from dr. @entity4 for his now well - established tendency to promote cure - alls more befitting 1915 than 2015 . turnabout is fair play . and @entity4 and his producers responded with alacrity , slicing and dicing his ill - prepared challengers with an investigative segment that would 've made \" @entity86 \" proud . he and his team score points with me for pointing out the media 's own failings in delightedly circulating the letter without looking into the backgrounds of anyone involved . it 's a simple matter to question ulterior motives when the letter itself takes pains to highlight @entity4 's critical attitude toward gmo foods , not one of his greatest indiscretions by a long shot . dr. @entity4 after all has conducted experiments on his tv audience , apparently in violation of the rules of his own academic medical center . he has a propensity to spout laughably definitive statements with little to no scientific support , such as his advice that \" every kid in @entity115 ought to be on @entity117 fatty acids and @entity118 from either the sun or a pill \" because this regimen will help them withstand concussions . instead of mounting a defense of the indefensible claims he delivers so easily and often , dr. @entity4 routed his critics by quickly pivoting to the undercurrents of their letter . he correctly pointed out that several of the letter 's signatories are @entity133 industry shills . one of the writers campaigned against a @entity136 proposition requiring @entity133 labeling , and one of the bunch even served time for felony medicaid fraud . these characters never stood a chance in tipping the scales against dr. @entity4 , but they got their headlines nonetheless . dr. @entity4 was able to transition their critique of his apparent disregard for science on his program into an easily vanquished attack on his straightforward stand for consumers ' right to accurate product labeling . when my wife first brought the letter to my attention , i immediately wanted to know whether these were @entity24 physicians . this whole affair would 've played out quite differently if a slate of credible colleagues based at his own institution were coming out against dr. oz. so far they 've made no demand for his resignation , though some colleagues made their discomfort public in an op - ed for @entity169 last week . dr. @entity4 is well - aware that some colleagues question him , discussing that tension in his @entity172 op - ed. he says he does n't expect all physicians to understand his approach to health promotion , where he 's willing to entertain just about everything , even seances . the closed , physician - only social network @entity179 issued dr. @entity4 numerous questions from its membership , none of which dr. @entity4 answered . they are revelatory of physician attitudes toward him nonetheless . one doc asked dr. @entity4 how he could keep up with the fast - changing world of cardiothoracic surgery and carry on with his show every weekday . another asked him how he knows so much about so many areas of medicine -- \" @entity193 certified n all these areas ? \" . both types of questions show the profound disconnect between most physicians , who tend not to speak unless they are certain in their expertise on a topic , and the way the media industry works . dr. @entity4 's show does n't require he stay up late at night prepping for the next day -- he has an office full of production staff behind him . let 's take it as a given that not every physician across @entity115 , or at @entity24 , has to agree with what dr. @entity4 says on his program . i certainly do n't . does he have the right to say it ? yes , but not without challenge . a real case can be made that dr. @entity4 has used his media megaphone to do harm as well as good . he is now a polarizing figure , and while @entity24 should be lauded for protecting the free speech of its academic staff , the equation with dr. @entity4 is becoming increasingly complex . he 's no longer simply good pr for the @entity24 and @entity25 , which is often featured in his show . the letter writers were correct about one thing : @entity24 's reputation is now linked with the @entity226 standing right out in front .\n",
      " Query: he says @entity4 scorned by some in medical community , at @placeholder hearing ; comics joke about him . he serves himself at cost to his hospital\n",
      " Answer: @entity30\n",
      " Expected: @entity30\n",
      "===============\n",
      "[2] Doc: ( @entity1 ) @entity0 is a cornerstone event in the @entity3 faith , but it 's surrounded by interesting quirks . it celebrates the completion of @entity8 's mission of salvation in the @entity10 and @entity11 . by dying on @entity12 , @entity8 atoned for the sins of others ; by rising from the grave on sunday , @entity8 conquered death . simple enough and reason for @entity18 to celebrate . but , like @entity19 with its tree , ornaments and @entity22 , @entity0 has picked up its peripheral trappings -- the bunny and colorful eggs . unlike @entity19 , it does n't fall on the same day every year but shifts around in spring depending upon cosmic events . and that blood moon we just had -- is it pure coincidence that it fell around @entity0 ? ( no. ) here 's a journey from the @entity32 to the @entity33 , around the moon and the @entity36 's tilting axis , to @entity37 and the @entity38 to try to explain the complex holiday called @entity0 . and you 'll learn to how to color easter eggs with kool - aid . let 's start at the @entity32 . at the @entity32 , @entity44 began with @entity45 last week and culminated in @entity46 with multiple celebrations in between to mark the final week of @entity8 's mortal life . @entity8 rode on the back of a donkey into @entity52 on @entity45 where crowds celebrated him as the @entity55 and laid down palm branches in his path . but the crowd and the @entity59 turned on him in the course of the week , according to the @entity61 , leading to his crucifixion and resurrection . rain sprinkled down on worshipers standing under a sea of umbrellas as they gathered in a gray @entity68 on sunday to partake in the outdoor services held by @entity71 . afterward , the @entity72 took a moment to tell the world to do right those in need in his @entity78 address . @entity71 lamented the suffering of people in many of the conflicts around the globe . from @entity84 to @entity85 , @entity86 to @entity87 , he expressed hope that violence would end . @entity71 also does n't put on airs . this week he washed the feet of believers , repeating the @entity61 account of a woman , a sinner , washing @entity8 ' feet and anointing them with oil . let 's move on to old @entity52 , the birthplace of @entity0 . the church of the @entity100 in old @entity52 unifies the spot where @entity8 was crucified -- @entity102 -- with his tomb , or sepulcher . on sunday , @entity105 and @entity106 @entity18 celebrated the resurrection there . in the morning , the @entity109 , the archbishop of @entity52 , entered the basilica . then mass was held followed by a procession . but a large group of indigenous @entity18 did n't join them . it 's not quite @entity0 yet in the @entity33 for @entity116 . they 'll be celebrating a week from now , because they determine @entity0 's date by a different calendar than @entity18 -- the @entity120 . which brings us to the question of how astronomy is used to determine the date of @entity46 . a blood moon appeared in the sky early saturday , right between @entity12 and @entity46 and during @entity128 . just a coincidence ? not completely , because the dates for both @entity128 -- the @entity132 holiday celebrating the deliverance from slavery in @entity135 -- and @entity0 are determined by moon phases , according to @entity137 . @entity0 's timing is related to @entity128 , because @entity8 was crucified around then , according to the @entity61 . many jewish holidays , including @entity128 , fall on full moon , which is also a prerequisite for a lunar eclipse , the event that turns the moon a blood red color . since the timing of moon phases does n't jibe with @entity36 's orbit -- which is how we determine the length of a year now -- @entity128 's exact date moves around -- and so does @entity0 's . when @entity3 bishops first convened at the @entity153 in the year 325 , they made a rule to determine the date of @entity0 , so as to fairly reliably pin it to @entity128 : it would fall on the first sunday after the first full moon following the vernal equinox . that 's the day in march when @entity36 's axis reaches a midpoint between winter and summer and the day and night are of equal length . but ... if the full moon fell on sunday , @entity0 would be pushed down a week . confusing ? it got worse . when the @entity164 moved from the @entity120 to the gregorian calendar , @entity116 stayed put , resulting in -- usually -- two separate dates for @entity0 . in 1997 , the @entity167 pushed for a unified method of determining a date based on astronomical occurrences . it did n't catch on . but some odd @entity0 trappings that popped up after the @entity173 very much have -- the eggs and the bunny . the bunny is an egg - laying pagan that worships the moon . that 's one notion of its origins , but probably not the actual one . @entity37 immigrants appear to have brought it to @entity185 in the 1700s . @entity37 historians are not clear on its beginnings , but the first known mention of the bunny and the eggs in writing was in 1682 . professor of medicine @entity195 described in his paper \" @entity198 ovis paschalibus , \" or \" on easter eggs , \" a custom in the @entity202 region involving a bunny and eggs , according to @entity37 public television . some also credit the region with inadvertently inventing the christmas tree . but @entity195 left out any explanation of how the tradition arose , leading to a number of theories in @entity37 . one common idea : during @entity214 , people had to abstain from eating eggs , but hens kept on laying them , so farmers boiled and preserved them . by the time @entity0 rolled around , they were practically swimming in them . they had to figure out something to do with them when the holiday hit . play hide and seek with them ; color them ; give them as gifts . parents may have invented the bunny as a playful explanation for children on where the @entity0 eggs came from . if you 're coloring eggs this year , here 's an interesting tip. instead of stinking up your place with the smell of vinegar , use @entity241 , @entity243 science geek @entity242 suggests . and it appears to work . just use a whole packet in a small glass of hot water and gently lay the eggs in . they turn out as bright as they would in any other food dye . but be careful , it stains everything else , like clothes and upholstery , @entity242 warns . that 's why your tongue changes colors when you drink it . happy @entity0 ! happy @entity128 !\n",
      " Query: @entity0 is a key event in the @placeholder faith , but where did the @entity265 come from ?\n",
      " Answer: @entity3\n",
      " Expected: @entity3\n",
      "===============\n",
      "[3] Doc: ( @entity5 ) for those wondering if we would ever hear from the @entity4 family again , the answer would appear to be yes . \" @entity7 \" executive producer @entity6 said the show will return for a fifth season of 17 episodes . the @entity12 mogul was interviewed on @entity14 ' podcast recently , and let it drop that fans can expect more of the quirky comedy . @entity18 had no comment for @entity5 when asked to verify his statements . the fourth season was streamed exclusively on @entity18 in 2013 , after @entity23 canceled the show several years before . despite critical acclaim , the series never had big ratings , but has a devoted fan base , who often quote from the show . it was not yet known if the full cast , including @entity33 , @entity34 and @entity35 , will return for the season .\n",
      " Query: fan favorite series \" @placeholder \" to return for a fifth season , according to producer\n",
      " Answer: @entity7\n",
      " Expected: @entity7\n",
      "===============\n",
      "[4] Doc: april 2 , 2015 an unstable @entity1 country has become a potential battlefield for a proxy war . today on @entity4 , hear an explainer on why @entity6 is the focus of global concern . we also report on the origins of @entity11 , we detail how a 1,000 - year - old recipe could cure a modern - day superbug , and we feature a @entity14 on a woman who 's steering kids to a better life . on this page you will find today 's show transcript and a place for you to request to be on the @entity22 . transcript click here to access the transcript of today 's @entity25 . please note that there may be a delay between the time when the video is available and when the transcript is published . @entity4 is created by a team of journalists who consider the @entity33 , national standards in different subject areas , and state standards when producing the show . @entity38 for a chance to be mentioned on the next @entity4 , comment on the bottom of this page with your school name , mascot , city and state . we will be selecting schools from the comments of the previous show . you must be a teacher or a student age 13 or older to request a mention on the @entity22 ! thank you for using @entity56 student news !\n",
      " Query: at the bottom of the page , comment for a chance to be mentioned on @entity4 . you must be a teacher or a student age 13 or older to request a mention on the @placeholder .\n",
      " Answer: @entity22\n",
      " Expected: @entity22\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import cntk.device as device\n",
    "import numpy as np\n",
    "import math\n",
    "from cntk import load_model\n",
    "\n",
    "def pred_cnn_model(model_path, output):\n",
    "  logger.init(\"cnn_test\")\n",
    "  if os.path.exists(output):\n",
    "    os.remove(output)\n",
    "  test_path = os.path.join(data_root, \"cnn/test.ctf\")\n",
    "  test_size=2291183\n",
    "  vocab_path = os.path.join(data_root, \"cnn/cnn.vocab\")\n",
    "  vocab_dim = 101585\n",
    "  entity_dim = 586\n",
    "  hidden_dim=256\n",
    "  max_rl_steps=5\n",
    "  embedding_dim=300\n",
    "  att_dim = 384\n",
    "  minibatch_size=1\n",
    "  share_rnn = True\n",
    "\n",
    "  test_data = create_reader(test_path, vocab_dim, entity_dim, False)\n",
    "  embedding_init = None\n",
    "\n",
    "  params = model_params(vocab_dim = vocab_dim, entity_dim = entity_dim, hidden_dim = hidden_dim,\n",
    "                        embedding_dim = embedding_dim, attention_dim=att_dim, max_rl_steps = max_rl_steps,\n",
    "                        embedding_init = embedding_init, dropout_rate = 0.2, share_rnn_param = share_rnn)\n",
    "\n",
    "  entity_table, word_table = Vocabulary.load_vocab(vocab_path)\n",
    "  model = load_model(model_path)\n",
    "  predict_func = predict(model, params)\n",
    "  bind = bind_data(predict_func, test_data)\n",
    "  context_stream = get_context_bind_stream(bind)\n",
    "  samples_sum = 0\n",
    "  i = 0\n",
    "  predicted_results = []\n",
    "  max_num = 5\n",
    "  start = os.times()\n",
    "  while i<test_size:\n",
    "    mbs = min(test_size - i, minibatch_size)\n",
    "    mb = test_data.next_minibatch(mbs, bind)\n",
    "    pred = predict_func.eval(mb)\n",
    "    ans = np.nonzero(pred)\n",
    "    for id in ans[1]:\n",
    "      predicted_results += [ entity_table.lookup_by_id(id) ]    \n",
    "    i += mb[context_stream].num_samples\n",
    "    samples = mb[context_stream].num_sequences\n",
    "    samples_sum += samples\n",
    "    sys.stdout.write('.')\n",
    "    sys.stdout.flush()\n",
    "    if samples_sum >= max_num:\n",
    "      break\n",
    "  end = os.times()\n",
    "  total = end.elapsed - start.elapsed\n",
    "  print(\"\")\n",
    "  print(\"Evaluated samples: {0} in {1} seconds\".format(samples_sum, total))\n",
    "  raw_test_path = os.path.join(data_root, \"cnn/test.txt\")\n",
    "  instance_id = 0\n",
    "  with open(raw_test_path, 'r', encoding='utf-8') as raw:\n",
    "    content = raw.readlines()\n",
    "    for record in content:\n",
    "      fields = record.strip().split('\\t')\n",
    "      query = fields[0]\n",
    "      answer = fields[1]\n",
    "      doc = fields[2]\n",
    "      print(\"===============\")\n",
    "      print(\"[{0}] Doc: {1}\\n Query: {2}\\n Answer: {3}\\n Expected: {4}\".\\\n",
    "            format(instance_id, doc, query, predicted_results[instance_id], answer))\n",
    "      \n",
    "      instance_id+=1\n",
    "      if instance_id >= len(predicted_results):\n",
    "        break\n",
    "pred_cnn_model(os.path.join(data_root, \"../model/model_training.ctf_001.dnn\"), \"pred.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix\n",
    "#### Mathematical details from the original paper\n",
    "\n",
    "\n",
    "\n",
    "In the ReasoNet paper, it gives the fomula of the Reward as\n",
    "\\begin{align}\n",
    "J(\\theta) = \\mathbf{E}_{\\pi\\left(t_{1:T},a_T;\\theta\\right)}\\left[\\sum_{t=1}^Tr_t\\right]\n",
    "\\end{align}\n",
    "\n",
    "And it applies REINFORCE algorithm to estimate \n",
    "\\begin{align} \n",
    "\\nabla_{\\theta}J(\\theta) = \\mathbf{E}_{\\pi\\left(t_{1:T},a_T;\\theta\\right)}\\left[\\nabla_{\\theta}log_{\\pi}\\left(t_{1:T},a_T;\\theta\\right)r_T\\right]=\\sum_{\\left(t_{1:T},a_T\\right)\\in\\mathbb{A}^+}\\pi\\left(t_{1:T},a_T;\\theta\\right)\\left[\\nabla_{\\theta}log\\pi\\left(t_{1:T},a_T;\\theta\\right)\\left(r_T-b_T\\right)\\right]\n",
    "\\end{align}\n",
    "\n",
    "However, as the baseline $\\left\\{b_T;T=1...T_{max}\\right\\}$ are global variables independent of instances, it leads to slow convergence in training ReasoNet. Instead, the paper rewrite the formular as,\n",
    "$$\n",
    "\\nabla_{\\theta}J(\\theta) =\\sum_{\\left(t_{1:T},a_T\\right)\\in\\mathbb{A}^+}\\pi\\left(t_{1:T},a_T;\\theta\\right)\\left[\\nabla_{\\theta}log\\pi\\left(t_{1:T},a_T;\\theta\\right)\\left(r_T-b\\right)\\right]\n",
    "$$\n",
    ",where $b=\\sum_{\\left(t_{1:T},a_T\\right)\\in\\mathbb{A}^+}\\pi\\left(t_{1:T},a_T;\\theta\\right)r_T$ is the average reward on the $\\left|\\mathbb{A}^+\\right|$ episodes.\n",
    "\n",
    "Since the sum of the rewards over $\\left|\\mathbb{A}^+\\right|$ episodes is zero, $\\sum_{\\left(t_{1:T},a_T\\right)\\in\\mathbb{A}^+}\\pi\\left(t_{1:T},a_T;\\theta\\right)\\left(r_T-b\\right)=0$, they call it Contrastive Reward. Further more, they found using $\\left(\\frac{r_T}{b}-1\\right)$ in replace of $\\left(r_T-b\\right)$ will lead to a better convergence.\n",
    "\n",
    "In our implementation, we take the reward in the form,\n",
    "$$\n",
    "J(\\theta)=\\sum_{\\left(t_{1:T},a_T\\right)\\in\\mathbb{A}^+}\\pi\\left(t_{1:T},a_T;\\theta\\right)\\left(\\frac{r_T}{b}-1\\right) + b\n",
    "$$\n",
    "As we only compute gradient on $\\pi\\left(t_{1:T},a_T;\\theta\\right)$ and treat other components in the formula as a constant, the derivate is the same as the paper while the output is the average rewards in $\\left|\\mathbb{A}^+\\right|$ episodes.\n",
    "In CNTK, we use stop_gradient operator over the output of a function to convert it to a constant in the math formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
